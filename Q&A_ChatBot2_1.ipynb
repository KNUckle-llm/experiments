{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOhFAhFTM/jmLw879oOZOL9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KNUckle-llm/experiments/blob/main/Q%26A_ChatBot2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEER2kRiIwvS"
      },
      "outputs": [],
      "source": [
        "# ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡ í”„ë¡œì íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s1WwBRU_Nq9y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "# ì¼ë‹¨ ì¢…í˜¸ì˜ openai API í‚¤ë¥¼ .env íŒŒì¼ì— ë„£ì–´ ë†“ìŒ => ë‚˜ì¤‘ì— ë°”ê¾¸ê¸°"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5KCM5kP6No8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y gradio\n",
        "#!pip install gradio==3.50.2 --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RG_3magVch41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai==0.3.7"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3R-NCn3dOPWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface==0.1.2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-Mw_Wf-zPhtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community==0.3.18"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hF1lOXXOPkQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu==1.10.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DW2y0rt0Rjjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D5quX3Ac8eVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°(openai API í‚¤)\n",
        "load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')\n",
        "\n",
        "# LLM ì„¤ì •\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë¶„ë¦¬\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "message = \"\"\"\n",
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
        "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥ì„ í† ëŒ€ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "ë§Œì•½, ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ìœ„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
        "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
        "\n",
        "## ì£¼ì–´ì§„ ë¬¸ë§¥:\n",
        "{context}\n",
        "\n",
        "## ì‚¬ìš©ì ì§ˆë¬¸:\n",
        "{input}\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", message)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ì¶œë ¥ íŒŒì„œ\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# ì „ì—­ ë³€ìˆ˜\n",
        "db = None\n",
        "retriever = None\n",
        "rag_chain = None\n",
        "faiss_path = \"/content/drive/MyDrive/FaissDB/knu_faiss_db\"\n",
        "\n",
        "# ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ë§Œë“¤ê¸°\n",
        "def setup_chain():\n",
        "    global retriever, rag_chain\n",
        "\n",
        "    retriever = db.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"k\": 3, \"fetch_k\": 10, \"lamda_mult\": 0.5}\n",
        "    )\n",
        "\n",
        "    rag_chain = {\n",
        "        \"context\": retriever,\n",
        "        \"input\": RunnablePassthrough()\n",
        "    } | prompt_template | llm | parser\n",
        "\n",
        "# ë“œë¼ì´ë¸Œì— ì¡´ì¬í•˜ëŠ” DB ë¡œë“œ\n",
        "def load_faiss_db():\n",
        "    global db\n",
        "    db = FAISS.load_local(\n",
        "        folder_path=faiss_path,\n",
        "        embeddings=hf_embeddings,\n",
        "        allow_dangerous_deserialization=True\n",
        "    )\n",
        "    setup_chain()\n",
        "\n",
        "# PDF ì—…ë¡œë“œ ë° DB ì €ì¥\n",
        "def add_pdf_to_db(file):\n",
        "    global db\n",
        "\n",
        "    loader = PyPDFLoader(file.name)\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "    # ê° ì²­í¬ì— íŒŒì¼ëª… metadata ì¶”ê°€\n",
        "    for doc in docs:\n",
        "        doc.metadata[\"file_name\"] = os.path.basename(file.name)\n",
        "        # ì—¬ê¸°ì— urlë„ ë„£ì„ ìˆ˜ ìˆì„ë“¯?\n",
        "\n",
        "    if db is None:\n",
        "        db = FAISS.from_documents(docs, hf_embeddings)\n",
        "    else:\n",
        "      db.add_documents(docs)\n",
        "\n",
        "    db.save_local(faiss_path)\n",
        "    setup_chain()\n",
        "\n",
        "    return f\"{os.path.basename(file.name)} ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ FAISS DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "# ì§ˆë¬¸ ì²˜ë¦¬\n",
        "def answer_question(question):\n",
        "    if rag_chain is None:\n",
        "        return \"ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”!\"\n",
        "    return rag_chain.invoke(question)\n",
        "\n",
        "def show_stored_documents():\n",
        "    if db is None:\n",
        "        return \"DB ë¡œë“œ ë¬¸ì œ\"\n",
        "\n",
        "    docs = list(db.docstore._dict.values())  # ì €ì¥ëœ ëª¨ë“  ì²­í¬ë“¤ì„ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "    file_names = {doc.metadata.get(\"file_name\", \"Unknown\") for doc in docs}\n",
        "    return \"ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡:\\n\" + \"\\n\".join(f\"â€¢ {f}\" for f in sorted(file_names))\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ğŸ“„ ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡\n",
        "    **ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤!**\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(label=\"PDF íŒŒì¼ ì„ íƒ\")\n",
        "            upload_button = gr.Button(\"ğŸ“¤ ë²¡í„° DBì— ì €ì¥\")\n",
        "            show_files_button = gr.Button(\"ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ë³´ê¸°\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            status_output = gr.Textbox(label=\"ğŸ“¢ ìƒíƒœ ë©”ì‹œì§€\")\n",
        "            question_input = gr.Textbox(label=\"â“ ì§ˆë¬¸ ì…ë ¥\", placeholder=\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì ì–´ì£¼ì„¸ìš”.\")\n",
        "            submit_button = gr.Button(\"ğŸ¤– ë‹µë³€ ë°›ê¸°\")\n",
        "            answer_output = gr.Textbox(label=\"ğŸ“ AI ë‹µë³€\")\n",
        "\n",
        "    upload_button.click(add_pdf_to_db, inputs=file_input, outputs=status_output)\n",
        "    submit_button.click(answer_question, inputs=question_input, outputs=answer_output)\n",
        "    show_files_button.click(show_stored_documents, outputs=status_output)\n",
        "\n",
        "# ë²¡í„° DB ë¡œë“œ í›„ ì‹¤í–‰\n",
        "load_faiss_db()\n",
        "demo.launch()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_FgPqUBrJUIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall numpy -y\n",
        "#!pip install --no-cache-dir numpy==1.26.4\n",
        "# colabì— numpy 2.ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì„œ ë²„ì „ ì¶©ëŒë‚¨"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qXZuuZ05P3qO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30e7bd4-79a6-4437-9c7e-3c5868727350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqCQ32ozSclI",
        "outputId": "456fbaae-eb29-4751-8eb1-259f9b72effa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "print(gradio.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObqvRExvFmXG",
        "outputId": "c48201ed-e567-48a9-e441-98e59836625d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.50.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colabì— ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
        "#from huggingface_hub import snapshot_download\n",
        "#snapshot_download(repo_id=\"BAAI/bge-m3\", local_dir=\"/content/drive/MyDrive/EmbeddingModel/bge-m3\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m6Lwb3SeW-L_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}