{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO11aQON2HAm/EYugpaMrzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KNUckle-llm/experiments/blob/main/Q%26A_ChatBot2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEER2kRiIwvS"
      },
      "outputs": [],
      "source": [
        "# ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡ í”„ë¡œì íŠ¸ 2-5 : 1í•™ê¸° ìµœì¢…ë³¸\n",
        "# ì¶”ê°€ì  1 : í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ë³€ê²½ (ì™„)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "s1WwBRU_Nq9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cf2924-8edb-463f-e4aa-379e47c2c888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5KCM5kP6No8S"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3R-NCn3dOPWk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_openai==0.3.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Mw_Wf-zPhtF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface==0.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hF1lOXXOPkQX"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community==0.3.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DW2y0rt0Rjjv"
      },
      "outputs": [],
      "source": [
        "#!pip install faiss-cpu==1.10.0\n",
        "!pip install langchain_chroma==0.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D5quX3Ac8eVU"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RG_3magVch41"
      },
      "outputs": [],
      "source": [
        "#!pip install pydantic==2.10.6\n",
        "!pip uninstall -y gradio\n",
        "!pip install --upgrade gradio gradio-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qXZuuZ05P3qO"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install --no-cache-dir numpy==1.26.4\n",
        "# colabì— numpy 2.ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì„œ ë²„ì „ ì¶©ëŒë‚¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqCQ32ozSclI",
        "outputId": "f6712287-b033-4df5-82a7-992cd8978f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObqvRExvFmXG",
        "outputId": "e2976266-416f-4eb7-98f4-7f7c2b8fb1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.33.0\n"
          ]
        }
      ],
      "source": [
        "import gradio\n",
        "print(gradio.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MXM0w5Q5t0c"
      },
      "outputs": [],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb1yms9cVH0Q"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-vision pdf2image\n",
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krtuPwiOIwfj"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import uuid\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°(openai API í‚¤)\n",
        "load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')\n",
        "\n",
        "# LLM ì„¤ì •\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë¶„ë¦¬\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "system_message = \"\"\"\n",
        "ë‹¹ì‹ ì€ ê³µì£¼ëŒ€í•™êµì— ê´€í•œ ì§ˆë¬¸ì— ì‘ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•˜ë©°, ë²¡í„° DBì— ì €ì¥ëœ ê³µì£¼ëŒ€í•™êµì˜ ê³µì‹ ë¬¸ì„œë§Œì„ ê·¼ê±°ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì‘ë‹µ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:\n",
        "\n",
        "1. ì§ˆë¬¸ì— ëª…ì‹œëœ ë¶€ì„œ ë˜ëŠ” ì‚¬ìš©ìê°€ ì„ íƒí•œ ë¶€ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
        "   - 'ALL(ì „ì²´)'ì¸ ê²½ìš°, ì§ˆë¬¸ ë‚´ìš©ì„ ë¶„ì„í•´ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¶€ì„œë¥¼ ìš°ì„  íŒë‹¨í•´ ì‘ë‹µí•˜ì„¸ìš”.\n",
        "   - ì´í›„ ëŒ€í™”ì—ì„œ ë¶€ì„œê°€ ì •ì •ë˜ë©´, í•´ë‹¹ ë¶€ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ ë‹µë³€í•˜ì„¸ìš”.\n",
        "\n",
        "2. ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, ë‹¤ìŒ ìš°ì„ ìˆœìœ„ë¥¼ ì§€í‚¤ì„¸ìš”:\n",
        "   - íŒŒì¼ëª… ë˜ëŠ” ë‚´ìš©ì— í¬í•¨ëœ **ì—°ë„ ê¸°ì¤€** ìµœì‹  ë¬¸ì„œ ìš°ì„ \n",
        "   - ê·¸ ì™¸ì—” **ì‘ì„±ì¼ ë˜ëŠ” ë‚´ìš© ì‹ ë¢°ë„** ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨\n",
        "\n",
        "3. ê´€ë ¨ ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ê²½ìš°, ë‹¤ìŒ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n",
        "   â€œí•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê³µì‹ ë¬¸ì„œë‚˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.â€\n",
        "\n",
        "4. ì¸ì‚¬, ê°ì‚¬, ì‚¬ìš©ë²• ë“± ì¼ìƒì  ì§ˆë¬¸ì€ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ë˜, ì¶œì²˜ëŠ” ìƒëµí•˜ê±°ë‚˜ â€œí•´ë‹¹ ì •ë³´ ì—†ìŒâ€ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.\n",
        "\n",
        "5. ëª¨ë“  ì‘ë‹µ í•˜ë‹¨ì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”:\n",
        "ì¶œì²˜:\n",
        "  - íŒŒì¼ëª…: â—‹â—‹â—‹ ë˜ëŠ” â€œí•´ë‹¹ ì •ë³´ ì—†ìŒâ€\n",
        "  - ë¶€ì„œ/í•™ê³¼: â—‹â—‹â—‹ ë˜ëŠ” â€œí•´ë‹¹ ì •ë³´ ì—†ìŒâ€\n",
        "  - URL: â—‹â—‹â—‹ ë˜ëŠ” â€œí•´ë‹¹ ì •ë³´ ì—†ìŒâ€\n",
        "\n",
        "â€» â€œNoneâ€, ë¹ˆ ë¬¸ìì—´, null ë“±ì€ ì ˆëŒ€ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ â€œí•´ë‹¹ ì •ë³´ ì—†ìŒâ€ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.\n",
        "\n",
        "ì˜ˆì‹œ:\n",
        "ë‹µë³€: â—‹â—‹â—‹\n",
        "ì¶œì²˜:\n",
        "  - íŒŒì¼ëª…: â—‹â—‹â—‹\n",
        "  - ë¶€ì„œ/í•™ê³¼: â—‹â—‹â—‹\n",
        "  - URL: â—‹â—‹â—‹\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_message),\n",
        "        (\"placeholder\", \"{memory}\"),\n",
        "        (\"human\", \"ì„ íƒëœ ë¶€ì„œ: {selected_dept}\\n\\nì§ˆë¬¸: {input}\\n\\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:\\n{context}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ì¶œë ¥ íŒŒì„œ\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# íŠ¸ë¦¬ë¨¸ ì„¤ì •\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=3500,\n",
        "    token_counter=llm,\n",
        "    strategy=\"last\",\n",
        "    include_system=True,\n",
        "    start_on=\"human\"\n",
        ")\n",
        "\n",
        "DEPARTMENT_MAP = {\n",
        "    # ëª¨ë“  ë¬¸ì„œ ë‹¤ ë•Œë ¤ë°•ìŒ\n",
        "    \"ALL(ì „ì²´)\" : \"knu_chroma_db_all\",\n",
        "\n",
        "    # ëŒ€í‘œ ì‚¬ì´íŠ¸\n",
        "    \"National Kongju University Korean Language Representative (êµ­ë¦½ê³µì£¼ëŒ€í•™êµ êµ­ë¬¸ ëŒ€í‘œ)\": \"knu_chroma_db_national_kongju_university_korean_language_representative\",\n",
        "\n",
        "    # ê³µê³¼ ëŒ€í•™ í•™ê³¼\n",
        "    \"Software Department (ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼)\": \"knu_chroma_db_software\",\n",
        "    \"Department of Computer Engineering (ì»´í“¨í„°ê³µí•™ê³¼)\": \"knu_chroma_db_computer\",\n",
        "    \"Department of Artificial Intelligence (ì¸ê³µì§€ëŠ¥í•™ë¶€)\": \"knu_chroma_db_artificial_intelligence\",\n",
        "    \"Department of Architecture (ê±´ì¶•í•™ê³¼)\": \"knu_chroma_db_architecture\",\n",
        "    \"Department of Chemical Engineering (í™”í•™ê³µí•™ë¶€)\": \"knu_chroma_db_chemical_engineering\",\n",
        "    \"Department of Design Convergence (ë””ìì¸ì»¨ë²„ì „ìŠ¤í•™ê³¼)\": \"knu_chroma_db_design_convergence\",\n",
        "    \"Department of Digital Convergence Mold Engineering (ë””ì§€í„¸ìœµí•©ê¸ˆí˜•ê³µí•™ê³¼)\": \"knu_chroma_db_digital_convergence_mold_engineering\",\n",
        "    \"Department of Electrical, Electronic and Control Engineering (ì „ê¸°ì „ìì œì–´ê³µí•™ë¶€)\": \"knu_chroma_db_electrical_electronic_and_control_engineering\",\n",
        "    \"Department of Electrical, Electronic and Control Engineering (ì „ê¸°ì „ìì œì–´ê³µí•™ë¶€)/Control and Instrumentation Engineering Major (ì œì–´ê³„ì¸¡ê³µí•™ì „ê³µ)\": \"knu_chroma_db_control_and_instrumentation_engineering_major\",\n",
        "    \"Department of Electrical, Electronic and Control Engineering (ì „ê¸°ì „ìì œì–´ê³µí•™ë¶€)/Electrical Engineering Major (ì „ê¸°ê³µí•™ì „ê³µ)\": \"knu_chroma_db_electrical_engineering_major\",\n",
        "    \"Department of Electrical, Electronic and Control Engineering (ì „ê¸°ì „ìì œì–´ê³µí•™ë¶€)/Electronic Engineering Major (ì „ìê³µí•™ì „ê³µ)\": \"knu_chroma_db_electronic_engineering_major\",\n",
        "    \"Department of Electrical, Electronic and Control Engineering (ì „ê¸°ì „ìì œì–´ê³µí•™ë¶€)/Semiconductor Information Engineering Major (ë°˜ë„ì²´ì •ë³´ê³µí•™ì „ê³µ)\": \"knu_chroma_db_semiconductor_information_engineering_major\",\n",
        "    \"Department of Environmental Engineering (í™˜ê²½ê³µí•™ê³¼)\": \"knu_chroma_db_environmental_engineering\",\n",
        "    \"Department of Future Automotive Engineering (ë¯¸ë˜ìë™ì°¨ê³µí•™ê³¼)\": \"knu_chroma_db_future_automotive_engineering\",\n",
        "    \"Department of Industrial Engineering (ì‚°ì—…ê³µí•™ê³¼)\": \"knu_chroma_db_industrial_engineering\",\n",
        "    \"Department of Information and Communication Engineering (ì •ë³´í†µì‹ ê³µí•™ê³¼)\": \"knu_chroma_db_information_and_communication_engineering\",\n",
        "    \"Department of Intelligent Mobility Engineering (ì§€ëŠ¥í˜•ëª¨ë¹Œë¦¬í‹°ê³µí•™ê³¼)\": \"knu_chroma_db_intelligent_mobility_engineering\",\n",
        "    \"Department of Mechanical and Automotive Engineering (ê¸°ê³„ìë™ì°¨ê³µí•™ë¶€)\": \"knu_chroma_db_mechanical_and_automotive_engineering\",\n",
        "    \"Department of New Materials Engineering (ì‹ ì†Œì¬ê³µí•™ë¶€)\": \"knu_chroma_db_new_materials_engineering\",\n",
        "    \"Department of Photonics (ê´‘ê³µí•™ê³¼)\": \"knu_chroma_db_photonics\",\n",
        "    \"Department of Smart Information Technology Engineering (ìŠ¤ë§ˆíŠ¸ì •ë³´ê¸°ìˆ ê³µí•™ê³¼)\": \"knu_chroma_db_smart_information_technology_engineering\",\n",
        "    \"Department of Smart Infrastructure Engineering (ìŠ¤ë§ˆíŠ¸ì¸í”„ë¼ê³µí•™ê³¼)\": \"knu_chroma_db_smart_infrastructure_engineering\",\n",
        "    \"Department of Urban and Transportation Engineering (ë„ì‹œÂ·êµí†µê³µí•™ê³¼)\": \"knu_chroma_db_urban_and_transportation_engineering\",\n",
        "    \"Green Smart Architectural Engineering Department (ê·¸ë¦°ìŠ¤ë§ˆíŠ¸ê±´ì¶•ê³µí•™ê³¼)\": \"knu_chroma_db_green_smart_architectural_engineering\",\n",
        "\n",
        "    # ëŒ€í•™ì›\n",
        "    \"Kongju National University Computer & Software & IT Convergence (êµ­ë¦½ê³µì£¼ëŒ€í•™êµ ì»´í“¨í„°&ì†Œí”„íŠ¸ì›¨ì–´&ITìœµí•©)\": \"knu_chroma_db_computer_software_it_convergence\",\n",
        "    \"All Graduate Schools (ì „ì²´ ëŒ€í•™ì›)\": \"knu_chroma_db_all_graduate_schools\",\n",
        "\n",
        "    # ì‚¬ì—…ë‹¨\n",
        "    \"National Kongju University SW Centered University Business Group (êµ­ë¦½ê³µì£¼ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨)\": \"knu_chroma_db_sw_centerd_university_business_group\",\n",
        "}\n",
        "\n",
        "# ì „ì—­\n",
        "retriever = None\n",
        "rag_chain = None\n",
        "history_store: dict[str, InMemoryChatMessageHistory] = {}\n",
        "\n",
        "# ì„¸ì…˜ ì´ˆê¸°í™” í•¨ìˆ˜\n",
        "def init_session(session_id: str):\n",
        "    if not session_id or not isinstance(session_id, str):\n",
        "        raise ValueError(\"ìœ íš¨í•œ session_idê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "    if session_id not in history_store:\n",
        "        history_store[session_id] = InMemoryChatMessageHistory()\n",
        "\n",
        "    # SystemMessageê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "    existing = history_store[session_id].messages\n",
        "    if not any(msg.type == \"system\" and msg.content.strip() == system_message.strip() for msg in existing):\n",
        "        history_store[session_id].add_message(SystemMessage(content=system_message.strip()))\n",
        "\n",
        "# ì„¸ì…˜ íˆìŠ¤í† ë¦¬ í•¨ìˆ˜\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    return history_store[session_id]\n",
        "\n",
        "def load_all_docs_once():\n",
        "    global all_docs\n",
        "    if not all_docs:\n",
        "        for dept, dir_name in DEPARTMENT_MAP.items():\n",
        "            db = Chroma(persist_directory=f\"/content/drive/MyDrive/ChromaDB/{dir_name}\", embedding_function=hf_embeddings)\n",
        "            res = db.get(include=[\"documents\", \"metadatas\"])\n",
        "            all_docs.extend([Document(page_content=d, metadata=m) for d, m in zip(res[\"documents\"], res[\"metadatas\"])])\n",
        "\n",
        "# RAG ì²´ì¸ ë¡œë“œ\n",
        "def rebuild_chain(selected_dept: str = None):\n",
        "    global retriever, rag_chain\n",
        "\n",
        "    # ì‚¬ìš©ìê°€ ë¶€ì„œë¥¼ ì„ íƒí•œ ê²½ìš°: í•´ë‹¹ ë¶€ì„œ ì „ìš© DBë§Œ ë¶ˆëŸ¬ì™€ retriever êµ¬ì„±(ë¹ ë¦„)\n",
        "    if selected_dept:\n",
        "        db_path = f\"/content/drive/MyDrive/ChromaDB/{DEPARTMENT_MAP[selected_dept]}\"\n",
        "        db = Chroma(persist_directory=db_path, embedding_function=hf_embeddings)\n",
        "    else:\n",
        "        # ë¶€ì„œë¥¼ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš°: ëª¨ë“  ë¶€ì„œì˜ ë¬¸ì„œê°€ ì €ì¥ëœ DBë¥¼ ë¡œë“œí•˜ì—¬ retriever êµ¬ì„±(ëŠë¦¼)\n",
        "        db_path = f\"/content/drive/MyDrive/ChromaDB/knu_chroma_db_all\"\n",
        "        db = Chroma(persist_directory=db_path, embedding_function=hf_embeddings)\n",
        "\n",
        "    \"\"\"\n",
        "    ë²¡í„° ê±°ë¦¬ ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰\n",
        "    ì§ˆë¬¸ -> MultiQuery(3ê°œì˜ ì„œë¸Œ ì§ˆë¬¸)\n",
        "    ì„œë¸Œ ì§ˆë¬¸ 1 -> ChromaDBì—ì„œ mmrë°©ì‹(ë‹¤ì–‘í•˜ê³ ë„ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë°©ì‹)ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 10ê°œ í›„ë³´ ì¤‘ 5ê°œë¥¼ ì„ íƒ\n",
        "    ì„œë¸Œ ì§ˆë¬¸ 2 -> ChromaDBì—ì„œ mmrë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 10ê°œ í›„ë³´ ì¤‘ 5ê°œë¥¼ ì„ íƒ\n",
        "    ì„œë¸Œ ì§ˆë¬¸ 3 -> ChromaDBì—ì„œ mmrë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 10ê°œ í›„ë³´ ì¤‘ 5ê°œë¥¼ ì„ íƒ\n",
        "\n",
        "    í‚¤ì›Œë“œ ê¸°ë°˜ ì¬ì •ë ¬ - Document ë¦¬ìŠ¤íŠ¸ ìƒíƒœì—ì„œ ì‘ë™í•¨\n",
        "    ì²­í¬ Nê°œë¥¼ ëŒ€ìƒìœ¼ë¡œ BM25(í‚¤ì›Œë“œ(=ì§ˆë¬¸ì—ì„œ ë‚˜ì˜¨ ë‹¨ì–´)ê°€ ë¬¸ì„œ ì•ˆì— ì–¼ë§ˆë‚˜ ì˜ ë“±ì¥í•˜ëŠ”ì§€ ê´€ë ¨ì„±ì„ ìˆ˜ì¹˜í™”í•˜ëŠ” ë°©ì‹)\n",
        "\n",
        "    ì•™ìƒë¸” -> top 3ê°œ ì„ íƒ\n",
        "    -> LLM contextì— top3 ì²­í¬ 3ê°œ ì‚½ì… -> ë‹µë³€ ìƒì„±\n",
        "    \"\"\"\n",
        "    # ì „ì²´ ë¬¸ì„œ ë¡œë”© í›„ BM25 êµ¬ì„±ìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ í™•ë³´\n",
        "    results = db.get(include=[\"documents\", \"metadatas\"])\n",
        "    docs = [Document(page_content=doc, metadata=meta) for doc, meta in zip(results[\"documents\"], results[\"metadatas\"])]\n",
        "\n",
        "    # Chroma + BM25 í˜¼í•© ê²€ìƒ‰ê¸° êµ¬ì„±\n",
        "    chroma_retriever = db.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"k\": 5, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
        "    )\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(docs)\n",
        "    bm25_retriever.k = 3\n",
        "    base_retriever = EnsembleRetriever(retrievers=[chroma_retriever, bm25_retriever])\n",
        "    retriever = MultiQueryRetriever.from_llm(base_retriever, llm=llm)\n",
        "\n",
        "    # ìµœì¢… RAG ì²´ì¸\n",
        "    rag_chain_core = {\n",
        "        \"memory\": trimmer,          # trimmer ì ìš©\n",
        "        \"context\": retriever,\n",
        "        \"input\": RunnablePassthrough(),\n",
        "        \"selected_dept\": RunnablePassthrough()\n",
        "    } | prompt_template | llm | parser\n",
        "\n",
        "    # ìµœì¢… RAG ì²´ì¸\n",
        "    rag_chain = RunnableWithMessageHistory(\n",
        "        rag_chain_core,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"memory\"\n",
        "    )\n",
        "\n",
        "# ì§ˆë¬¸ ì²˜ë¦¬\n",
        "def answer_question(question: str, session_id: str, selected_dept: str) -> str:\n",
        "    if not session_id:\n",
        "        return \"âš ï¸ ì„¸ì…˜ì„ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "    init_session(session_id)\n",
        "\n",
        "    # rag_chainì˜ ì…ë ¥ í˜•ì‹ì— ë§ì¶° dict êµ¬ì„±\n",
        "    chain_input = {\n",
        "        \"memory\": None,        # RunnableWithMessageHistoryê°€ ì•Œì•„ì„œ ì²˜ë¦¬\n",
        "        \"context\": None,       # retrieverê°€ ì•Œì•„ì„œ ì²˜ë¦¬\n",
        "        \"input\": question,\n",
        "        \"selected_dept\": selected_dept if selected_dept else \"ALL(ì „ì²´)\"\n",
        "    }\n",
        "\n",
        "    # RAG ì²´ì¸ í˜¸ì¶œ â†’ ë‹µë³€ ìƒì„±(ìë™ìœ¼ë¡œ historyì— ê¸°ë¡ë¨)\n",
        "    answer = rag_chain.invoke(\n",
        "        chain_input,\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# ì—¬ëŸ¬ê°œì˜ Chroma DBì— ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ -> ì• ë„ ì•ˆì”€\n",
        "def show_stored_documents():\n",
        "    file_names = set()\n",
        "\n",
        "    try:\n",
        "        for dept, dir_name in DEPARTMENT_MAP.items():\n",
        "            db = Chroma(\n",
        "                persist_directory=f\"/content/drive/MyDrive/ChromaDB/{dir_name}\",\n",
        "                embedding_function=hf_embeddings\n",
        "            )\n",
        "            result = db.get(include=[\"metadatas\"])\n",
        "            metas = result.get(\"metadatas\", [])\n",
        "            for meta in metas:\n",
        "                file_name = meta.get(\"file_name\", \"Unknown\")\n",
        "                department = meta.get(\"department\", dept)\n",
        "                file_names.add(f\"{file_name} ({department})\")\n",
        "    except Exception as e:\n",
        "        return f\"â— DBì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
        "\n",
        "    if not file_names:\n",
        "        return \"ğŸ“‚ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    return \"ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡:\\n\" + \"\\n\".join(f\"â€¢ {f}\" for f in sorted(file_names))\n",
        "\n",
        "# ì„¸ì…˜ë³„ë¡œ ì €ì¥ëœ ëŒ€í™” ë‚´ì—­ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜(Human/AI)\n",
        "def show_history(session_id: str) -> str:\n",
        "    init_session(session_id)\n",
        "    msgs = get_session_history(session_id).messages\n",
        "\n",
        "    if not msgs:\n",
        "        return \"âš ï¸ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    output = []\n",
        "    for msg in msgs:\n",
        "        if msg.type == \"system\":\n",
        "            output.append(f\"ğŸ“˜ System: {msg.content.strip()}\")\n",
        "        elif msg.type == \"human\":\n",
        "            output.append(f\"ğŸ§‘ Human: {msg.content.strip()}\")\n",
        "        elif msg.type == \"ai\":\n",
        "            output.append(f\"ğŸ¤– AI: {msg.content.strip()}\")\n",
        "    return \"\\n\\n\".join(output)\n",
        "\n",
        "# ì•± ì‹œì‘ ì‹œ ê¸°ë³¸ ì„¸ì…˜ ìƒì„± ë° db ë¡œë“œ\n",
        "default_session_id = f\"session_{uuid.uuid4().hex[:8]}\"\n",
        "init_session(default_session_id)\n",
        "rebuild_chain(\"ALL(ì „ì²´)\")\n",
        "\n",
        "# Gradio UI ì„¤ì •\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # â”€â”€ ì‚¬ì´ë“œë°”: ì„¸ì…˜ ê´€ë¦¬ â”€â”€\n",
        "    with gr.Sidebar():\n",
        "        session_dropdown = gr.Dropdown(\n",
        "            label=\"ì±„íŒ… ì„¸ì…˜ ì„ íƒ\",\n",
        "            choices=[default_session_id],\n",
        "            value=default_session_id,\n",
        "            interactive=True\n",
        "        )\n",
        "        new_session_btn = gr.Button(\"â• ìƒˆ ì±„íŒ…\")\n",
        "        del_session_btn = gr.Button(\"ğŸ—‘ ì„¸ì…˜ ì‚­ì œ\")\n",
        "\n",
        "        department_dropdown = gr.Dropdown(\n",
        "            label=\"ğŸ” ê²€ìƒ‰í•  ë¶€ì„œ/í•™ê³¼ ì„ íƒ\",\n",
        "            choices=list(DEPARTMENT_MAP.keys()),\n",
        "            value=\"ALL(ì „ì²´)\",\n",
        "            interactive=True\n",
        "        )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ğŸ“„ ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡\n",
        "    **ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤!**\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            show_files_button = gr.Button(\"ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ë³´ê¸°\")\n",
        "            status_output = gr.Textbox(label=\"ğŸ“¢ ìƒíƒœ ë©”ì‹œì§€\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            question_input = gr.Textbox(label=\"â“ ì§ˆë¬¸ ì…ë ¥\", placeholder=\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì ì–´ì£¼ì„¸ìš”.\")\n",
        "            submit_button = gr.Button(\"ğŸ¤– ë‹µë³€ ë°›ê¸°\")\n",
        "            answer_output = gr.Textbox(label=\"ğŸ“ AI ë‹µë³€\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            show_history_button = gr.Button(\"ğŸ•˜ íˆìŠ¤í† ë¦¬ ë³´ê¸°\")\n",
        "            history_output = gr.Textbox(label=\"ğŸ—’ ì „ì²´ ëŒ€í™” ë‚´ì—­ | System/Human/AI Message\", lines=15, interactive=False)\n",
        "\n",
        "    # â”€â”€ ì‚¬ì´ë“œë°” ì´ë²¤íŠ¸ ë°”ì¸ë”© â”€â”€\n",
        "    def create_session():\n",
        "        sid = f\"session_{uuid.uuid4().hex[:8]}\"\n",
        "        init_session(sid)\n",
        "        rebuild_chain(\"ALL(ì „ì²´)\")  # ì„¸ì…˜ë§ˆë‹¤ RAG ì´ˆê¸°í™”\n",
        "        return gr.update(choices=list(history_store.keys()), value=sid)\n",
        "\n",
        "    # ì„¸ì…˜ ì‚­ì œ í•¨ìˆ˜\n",
        "    def delete_session(session_id: str):\n",
        "        history_store.pop(session_id, None)\n",
        "\n",
        "    new_session_btn.click(\n",
        "        fn=create_session,\n",
        "        inputs=None,\n",
        "        outputs=[session_dropdown]\n",
        "    )\n",
        "\n",
        "    def remove_session(sid):\n",
        "        delete_session(sid)\n",
        "        keys = list(history_store.keys())\n",
        "        new_val = keys[0] if keys else None\n",
        "        return gr.update(choices=keys, value=new_val)\n",
        "\n",
        "    del_session_btn.click(\n",
        "        fn=remove_session,\n",
        "        inputs=[session_dropdown],\n",
        "        outputs=[session_dropdown]\n",
        "    )\n",
        "\n",
        "    # â”€â”€ ë¶€ì„œ ì„ íƒ ì‹œ RAG ì²´ì¸ ì¬êµ¬ì„± ì´ë²¤íŠ¸ ë°”ì¸ë”© â”€â”€\n",
        "    def on_department_selected(dept_name):\n",
        "        rebuild_chain(dept_name)\n",
        "        return f\"âœ… '{dept_name}' ë¶€ì„œ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ê¸°ë¥¼ ì¬ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    department_dropdown.change(\n",
        "        fn=on_department_selected,\n",
        "        inputs=[department_dropdown],\n",
        "        outputs=[status_output]\n",
        "    )\n",
        "\n",
        "    # â”€â”€ íŒŒì¼ ì—…ë¡œë“œ / ì§ˆë¬¸ / íˆìŠ¤í† ë¦¬ ì´ë²¤íŠ¸ â”€â”€\n",
        "    submit_button.click(answer_question, inputs=[question_input, session_dropdown, department_dropdown], outputs=answer_output)\n",
        "    show_files_button.click(show_stored_documents, outputs=status_output)\n",
        "    show_history_button.click(show_history, inputs=[session_dropdown], outputs=history_output)\n",
        "\n",
        "# ì•± ì‹¤í–‰\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7YUqoagH4Vr"
      },
      "outputs": [],
      "source": [
        "# ë“œë¼ì´ë¸Œ ë‚´ í´ë” ì•ˆì— ì¡´ì¬í•˜ëŠ” pdf íŒŒì¼ë“¤ ì €ì¥í•˜ëŠ” ì½”ë“œ\n",
        "# íë¦„: í´ë” ìˆœíšŒ -> pdf/md íŒŒì¼ 1ê°œ ì°¾ê¸° â†’ URL ë§¤ì¹­(ìœ ì‚¬ë„ ê¸°ë°˜) : íŒŒì¼ëª…ì„ ê°€ì§€ê³  URL ì•Œì•„ì˜¤ê¸° -> í…ìŠ¤íŠ¸ load & split(ì²­í‚¹) â†’ ë©”íƒ€ ì •ë³´ ì¶”ê°€ -> ChromaDBì— ì €ì¥(ìë™ ì„ë² ë”©)\n",
        "# íë¦„ì„ ë°˜ë³µí•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import logging\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "from io import BytesIO\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "# from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from google.cloud import vision\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# === Google Vision API ì„¤ì • === #\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Colab Notebooks/lively-sentry-460812-j2-b70abbe49963.json\"\n",
        "vision_client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# â€” ì„¤ì • â€” #\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/National Kongju University Korean Language Representative (êµ­ë¦½ê³µì£¼ëŒ€í•™êµ êµ­ë¬¸ ëŒ€í‘œ)\"   # ìˆœíšŒí•  ìµœìƒìœ„ í´ë”\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_all\"         # ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬\n",
        "EMB_MODEL     = \"BAAI/bge-m3\"\n",
        "URL_XLSX_PATH = os.path.join(ROOT_FOLDER, \"\")\n",
        "SIMILARITY_CUTOFF = 0.7\n",
        "\n",
        "# PyPDF ê²½ê³  ë¬´ì‹œ\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
        "\n",
        "# â€” ë¡œê¹… ì„¤ì • (Colabì—ì„œë„ ë³´ì´ê²Œ) â€” #\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "handler.setFormatter(formatter)\n",
        "if not logger.hasHandlers():\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "# íŒŒì¼ëª… ì •ê·œí™”: NFC â†’ í™•ì¥ì ì œê±° â†’ ê³µë°± ì œê±°\n",
        "def normalize_name(name: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFC\", name)\n",
        "    s = re.sub(r\"\\.\\w+$\", \"\", s)  # .pdf, .md ë“± ì œê±°\n",
        "    return s.replace(\" \", \"\")\n",
        "\n",
        "# 1) URL ë§¤í•‘ ë¡œë“œ\n",
        "def load_url_map(path: str) -> dict:\n",
        "    df = pd.read_excel(path)\n",
        "    return {\n",
        "        normalize_name(str(fn)): url\n",
        "        for fn, url in zip(df[\"íŒŒì¼ëª…\"], df[\"URL\"])\n",
        "    }\n",
        "if os.path.isfile(URL_XLSX_PATH):\n",
        "    URL_MAP = load_url_map(URL_XLSX_PATH)\n",
        "    logger.info(\"ğŸ”— URL ë§¤í•‘ ì—‘ì…€ ë¡œë“œ ì™„ë£Œ\")\n",
        "else:\n",
        "    logger.warning(\"âš ï¸ URL ë§¤í•‘ ì—‘ì…€ ì—†ìŒ\")\n",
        "    URL_MAP = {}\n",
        "\n",
        "# 2) ê°€ì¥ ìœ ì‚¬í•œ URL ì°¾ì•„ì£¼ëŠ” í•¨ìˆ˜\n",
        "def find_best_url(base_norm: str, url_map: dict, cutoff: float) -> str:\n",
        "    # ì™„ì „ ì¼ì¹˜\n",
        "    if base_norm in url_map:\n",
        "        logger.info(f\"ğŸ”— URL 100% ì¼ì¹˜: '{base_norm}' â†’ {url_map[base_norm]}\")\n",
        "        return url_map[base_norm]\n",
        "\n",
        "    # ìœ ì‚¬ë„ íƒìƒ‰\n",
        "    best_key, best_score = None, 0.0\n",
        "    for key in url_map:\n",
        "        score = SequenceMatcher(None, base_norm, key).ratio()\n",
        "        if score > best_score:\n",
        "            best_key, best_score = key, score\n",
        "\n",
        "    if best_score >= cutoff:\n",
        "        logger.info(f\"ğŸ” ìœ ì‚¬ë„({best_score:.2f}) ë§¤ì¹­: '{base_norm}' â†’ '{best_key}' â†’ {url_map[best_key]}\")\n",
        "        return url_map[best_key]\n",
        "    else:\n",
        "        logger.info(f\"âŒ URL ë§¤ì¹­ ì‹¤íŒ¨({best_score:.2f}): '{base_norm}' â†’ URL ì—†ìŒ\")\n",
        "        return \"\"\n",
        "\n",
        "# 3) Loader ì„ íƒ (.pdf/.md)\n",
        "def get_loader(fp: Path):\n",
        "    ext = fp.suffix.lower()\n",
        "    if ext == \".pdf\":\n",
        "        return PyPDFLoader(str(fp))\n",
        "    if ext == \".md\":\n",
        "        return TextLoader(str(fp), encoding=\"utf-8\")\n",
        "    return None\n",
        "\n",
        "# OCR ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def ocr_pdf_to_text(pdf_path: str) -> str:\n",
        "    text_list = []\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path)\n",
        "        for i, image in enumerate(images):\n",
        "            img_byte_arr = BytesIO()\n",
        "            image.save(img_byte_arr, format='JPEG')\n",
        "            content = img_byte_arr.getvalue()\n",
        "            image_vision = vision.Image(content=content)\n",
        "            response = vision_client.document_text_detection(image=image_vision)\n",
        "            text = response.full_text_annotation.text\n",
        "            logger.info(f\"ğŸ“ OCR ì¶”ì¶œ (p.{i+1}): {len(text)}ì\")\n",
        "            text_list.append(text)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "    return \"\\n\".join(text_list)\n",
        "\n",
        "# 4) Splitter & Embedding ì¤€ë¹„\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        ")\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "\n",
        "# 5) ChromaDB ë¡œë“œ (í´ë”ê°€ ì—†ìœ¼ë©´ ë‚´ë¶€ì ìœ¼ë¡œ ìƒˆë¡œ ìƒì„±ë¨)\n",
        "db = Chroma(\n",
        "    persist_directory=PERSIST_DIR,\n",
        "    embedding_function=hf_embeddings\n",
        ")\n",
        "logger.info(\"ğŸ”„ ChromaDB ë¡œë“œ/ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "\n",
        "# 6) íŒŒì¼ ìˆœíšŒí•˜ë©° ì²­í‚¹ + ë©”íƒ€ ì¶”ê°€ + DB ì €ì¥\n",
        "for fp in Path(ROOT_FOLDER).rglob(\"*.*\"):\n",
        "    loader = get_loader(fp)\n",
        "    if loader is None:\n",
        "        continue\n",
        "\n",
        "    base_raw  = fp.name\n",
        "    base_norm = normalize_name(base_raw)\n",
        "    # department ì¶”ì¶œ (í˜„ì¬ ë£¨íŠ¸ í´ë” ê¸°ì¤€)\n",
        "    department = Path(ROOT_FOLDER).name\n",
        "\n",
        "    # ì¤‘ë³µ ì²´í¬: file_nameê³¼ departmentê°€ ëª¨ë‘ ì¼ì¹˜í•  ê²½ìš°ì—ë§Œ ì¤‘ë³µ ê°„ì£¼\n",
        "    try:\n",
        "        results = db.get(where={\"file_name\": {\"$eq\": base_raw}})\n",
        "        metadatas = results.get(\"metadatas\", [])\n",
        "        if any(md.get(\"department\") == department for md in metadatas):\n",
        "            logger.info(f\"â­ï¸ ì´ë¯¸ ì €ì¥ëœ íŒŒì¼: {base_raw} ({department}) â†’ ìŠ¤í‚µ\")\n",
        "            continue\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"â— ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}\")\n",
        "\n",
        "    logger.info(f\"ğŸ“„ íŒŒì¼ ë¡œë“œ & ì²­í‚¹ ì‹œì‘: {fp.relative_to(ROOT_FOLDER)}\")\n",
        "    url = find_best_url(base_norm, URL_MAP, SIMILARITY_CUTOFF)\n",
        "\n",
        "    # ë¡œë“œ & ì²­í‚¹\n",
        "    try:\n",
        "        chunks = loader.load_and_split(text_splitter=text_splitter)\n",
        "        logger.info(f\"ğŸ§© ì²­í¬ ìƒì„±: {len(chunks)}ê°œ\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ '{base_raw}' ë¡œë“œ/ì²­í‚¹ ì‹¤íŒ¨: {e}\")\n",
        "        continue\n",
        "\n",
        "    # ì²­í¬ê°€ 0ê°œë©´ ì‚¬ì§„ìœ¼ë¡œëœ pdfë¬¸ì„œì´ë¯€ë¡œ OCR ì‹œë„\n",
        "    if not chunks and fp.suffix.lower() == \".pdf\":\n",
        "        logger.warning(f\"âš ï¸ '{base_raw}' ë¹ˆ ì²­í¬ â†’ ì‚¬ì§„ìœ¼ë¡œ ëœ pdf\")\n",
        "        logger.info(f\"ğŸ–¼ï¸ OCR ì²˜ë¦¬ ì‹œë„: {base_raw}\")\n",
        "        ocr_text = ocr_pdf_to_text(str(fp))\n",
        "        if ocr_text.strip():\n",
        "            chunks = text_splitter.create_documents([ocr_text])\n",
        "        else:\n",
        "            logger.warning(f\"âš ï¸ OCR ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼: {base_raw}\")\n",
        "            continue\n",
        "\n",
        "    # ë©”íƒ€ ì¶”ê°€\n",
        "    for chunk in chunks:\n",
        "        chunk.metadata.update({\n",
        "            \"file_name\":  base_raw,\n",
        "            \"department\": department,\n",
        "            \"url\":        url\n",
        "        })\n",
        "\n",
        "    # DB ì €ì¥\n",
        "    db.add_documents(chunks)\n",
        "    logger.info(f\"â• '{base_raw}' â€” {len(chunks)}ê°œ Chroma DBì— ì €ì¥ ì™„ë£Œ\")\n",
        "\n",
        "logger.info(\"ğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ â€” ChromaDBì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
        "count = db._collection.count()\n",
        "logger.info(f\"í˜„ì¬ DB ì²­í¬í™”ëœ ë¬¸ì„œ ê°œìˆ˜: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQW_Hdd07pWS"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# â€” ì„¤ì • â€” #\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_national_kongju_university_korean_language_representative\"\n",
        "EMB_MODEL   = \"BAAI/bge-m3\"\n",
        "\n",
        "# 1) DB ë¡œë“œ\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=hf_embeddings)\n",
        "\n",
        "# 2) ë©”íƒ€ì •ë³´ë§Œ êº¼ë‚´ì˜¤ê¸°\n",
        "res = db.get(include=[\"metadatas\"])\n",
        "\n",
        "# 3) ì´ ì²­í¬ ìˆ˜ ë° íŒŒì¼ëª…Â·ë¶€ì„œÂ·URL ì¶œë ¥\n",
        "metas = res[\"metadatas\"]\n",
        "print(f\"ì´ ì²­í¬ ìˆ˜: {len(metas)}\\n\")\n",
        "for meta in metas:\n",
        "    print(\n",
        "        f\"íŒŒì¼ëª…: {meta.get('file_name','Unknown')}  |  \"\n",
        "        f\"ë¶€ì„œ: {meta.get('department','Unknown')}  |  \"\n",
        "        f\"URL: {meta.get('url','')}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "bQ-uYzmm_4XX",
        "outputId": "5f50493e-d5ca-44b2-fc53-9f847b7490f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport shutil\\n\\nPERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_smart_infrastructure_engineering\"\\n\\n# 1) í´ë” ìì²´ë¥¼ ë‚ ë ¤ë²„ë¦¬ê¸°\\nshutil.rmtree(PERSIST_DIR)\\n\\nprint(f\"ğŸ—‘ï¸ \\'{PERSIST_DIR}\\' í´ë”ë¥¼ ì™„ì „íˆ ì œê±°í–ˆìŠµë‹ˆë‹¤.\")\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "import shutil\n",
        "\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_smart_infrastructure_engineering\"\n",
        "\n",
        "# 1) í´ë” ìì²´ë¥¼ ë‚ ë ¤ë²„ë¦¬ê¸°\n",
        "shutil.rmtree(PERSIST_DIR)\n",
        "\n",
        "print(f\"ğŸ—‘ï¸ '{PERSIST_DIR}' í´ë”ë¥¼ ì™„ì „íˆ ì œê±°í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sze8a2wqUg8m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db\"\n",
        "EMB_MODEL   = \"BAAI/bge-m3\"\n",
        "\n",
        "# DB ë¡œë“œ\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=hf_embeddings)\n",
        "\n",
        "# 1. where ì ˆë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§\n",
        "try:\n",
        "    results = db.get(\n",
        "        where={\"department\": {\"$eq\": \"Department of Computer Engineering (ì»´í“¨í„°ê³µí•™ê³¼)\"}},\n",
        "        include=[\"metadatas\", \"documents\"]  # includeì— idsëŠ” ì“°ë©´ ì•ˆ ë¨\n",
        "    )\n",
        "    ids_to_delete = results[\"ids\"]  # ì—¬ê¸°ì„œ idê°€ ìë™ í¬í•¨ë¨\n",
        "\n",
        "    if ids_to_delete:\n",
        "        db.delete(ids=ids_to_delete)\n",
        "        print(f\"ğŸ—‘ï¸ ì‚­ì œëœ ì²­í¬ ìˆ˜: {len(ids_to_delete)}\")\n",
        "    else:\n",
        "        print(\"âœ… ì‚­ì œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "except Exception as e:\n",
        "    print(f\"â— ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGM-u2-qU6X6"
      },
      "outputs": [],
      "source": [
        "\"\"\" OCR ì‚¬ìš© ë°©ë²• ì˜ˆì œ\n",
        "from google.colab import drive\n",
        "import os\n",
        "from google.cloud import vision\n",
        "from pdf2image import convert_from_path\n",
        "from io import BytesIO\n",
        "\n",
        "# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œ\n",
        "# êµ¬ê¸€ ë¹„ì „ OCR api í‚¤\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Colab Notebooks/lively-sentry-460812-j2-b70abbe49963.json\"\n",
        "\n",
        "# Vision API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
        "vision_client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# OCR ëŒ€ìƒ PDF ê²½ë¡œ\n",
        "pdf_path = \"/content/drive/MyDrive/Cheonan Campus All Departments (ì²œì•ˆìº í¼ìŠ¤ ëª¨ë“  í•™ê³¼)/Software Department (ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼)/ì»¤ë®¤ë‹ˆí‹° (íŠ¹ê°•ê³µì§€)/ë³´ë¬¼ì˜ ì„¸ê³„ ì´ë²¤íŠ¸.pdf\"\n",
        "\n",
        "# PDF â†’ ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
        "images = convert_from_path(pdf_path)\n",
        "\n",
        "# ğŸš— 6. OCR ì²˜ë¦¬\n",
        "for i, image in enumerate(images):\n",
        "    img_byte_arr = BytesIO()\n",
        "    image.save(img_byte_arr, format='JPEG')\n",
        "    content = img_byte_arr.getvalue()\n",
        "\n",
        "    image_vision = vision.Image(content=content)\n",
        "    response = vision_client.document_text_detection(image=image_vision)\n",
        "    text = response.full_text_annotation.text\n",
        "    print(f\"ğŸ“ [í˜ì´ì§€ {i+1}] OCR ê²°ê³¼:\")\n",
        "    print(text)\n",
        "    print(\"-\" * 50)\n",
        "\"\"\""
      ]
    }
  ]
}