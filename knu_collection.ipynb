{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMGh23IBaJHw9+HeRqVA3dB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KNUckle-llm/experiments/blob/main/knu_collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word -> pdf 파일로 변환(로컬)\n",
        "# Excel -> openpyxl로 텍스트 추출 후 저장\n",
        "# HWP -> PDF 파일로 변환(로컬)\n",
        "# ppt -> PDF 파일로 변환(로컬)\n",
        "# 모든 html 페이지는 md로 저장\n",
        "# jpg, pdf(사진) -> ocr로 텍스트 추출 후 저장\n",
        "\n",
        "\"\"\"\n",
        "앞으로 할일\n",
        "# 워드 파일 -> pdf 파일로 변환 예시(완료)\n",
        "# 엑셀 파일 -> pdf 변환 해야함(완료)\n",
        "# pdf(사진) -> pdfplumber로도 추출이 안되면 사진이니 ocr 진행하기(완료)\n",
        "# jpg, png 사진파일 -> 텍스트 추출 후 저장 (완료)\n",
        "전처리 + 문서 청킹(나누기) 방법 (진행 중)\n",
        "\n",
        "→ 학과는 너무 작게 나눔, 캠퍼스별로 collection을 나누고, 질문 전에 해당 collection만 검색.\n",
        "사용자 질문 -> 잘 변환 -> 해당되는 collection을 불러와 안에 청크들을 조회하며 유사도 높은걸 검색한다.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JFTEu-E4kPb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R-CMJHq6cs38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf9c476-3050-4d58-e573-867395bef062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 라이브러리 설치\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers\n",
        "!pip install pandas\n",
        "!pip install PyPDF2\n",
        "!pip install pdfplumber\n",
        "\n",
        "# 엑셀 텍스트 추출\n",
        "!pip install openpyxl\n",
        "\n",
        "# ocr PaddleOCR 사용\n",
        "!pip install paddleocr\n",
        "!pip install \"paddlepaddle==2.6.2\" -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "HMBZE6WnnMvn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# transformers 버전이 꼬이면 오류나면 다시 설치\n",
        "# 1. transformers 삭제\n",
        "!pip uninstall -y transformers\n",
        "\n",
        "# 2. 안정적인 버전으로 재설치 (4.40.2 또는 4.41.1 권장)\n",
        "!pip install transformers==4.41.2\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TEvtLoKP1mwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain 아님\n",
        "# 구글 드라이브 document_files에 있는 파일들 전부 저장\n",
        "# 경고 및 오류 처리 방법\n",
        "# incorrect startxref pointer(1) : Xref Table (객체 위치 정보) 를 찾기 위한 포인터가 잘못되었거나 손상된 경우 나타나는 경고, 되긴하나 일부 페이지나 객체 누락될 수 있음 -> 해결책 pdfplumber\n",
        "# Advanced encoding /KSCms-UHC-H not implemented yet : DF가 한글 문자셋(KSCms-UHC-H) 을 사용하는데, PyPDF2 라이브러리는 이 고급 인코딩을 완벽하게 지원하지 않는다는 경고 -> 해결책 pdfplumber\n",
        "# ⚠️ 2020-1 수강신청 안내문(컴퓨터공학전공).pdf: 텍스트 없음 → 스킵 pdf가 사진으로 되어 있는 경우 발생하는 문제 -> 해결책 ocr\n",
        "import io, os, re, unicodedata, pytz, warnings, logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pdfplumber\n",
        "from datetime import datetime\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb import PersistentClient\n",
        "from paddleocr import PaddleOCR\n",
        "from pdf2image import convert_from_path\n",
        "from difflib import get_close_matches, SequenceMatcher\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pdfminer')\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "\n",
        "# 설정\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/chroma_index\"\n",
        "campus_name = \"cheonan\"  # 또는 \"gongju\", \"yesan\", cheonan\n",
        "COLLECTION_NAME = f\"knu_{campus_name}_collection\"\n",
        "# 과거: COLLECTION_NAME = \"knu_collection\" 현재 : 캠퍼스별로 컬렉션 생성\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/document files Cheonan Campus\"     # 여기만 바꾸기(경로)\n",
        "URL_MAPPING_SUFFIX = \"_url.xlsx\"\n",
        "\n",
        "# PaddleOCR 초기화 (최초 1회만)\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='korean')\n",
        "\n",
        "KST = pytz.timezone('Asia/Seoul')\n",
        "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "client = PersistentClient(path=PERSIST_DIR)\n",
        "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "existing_ids = set(collection.get(limit=None)['ids'])\n",
        "\n",
        "# 텍스트 전처리 함수\n",
        "def clean_text(text):\n",
        "    # 공백, 특수문자 정리 + 너무 짧은 줄 제거\n",
        "    text = unicodedata.normalize(\"NFC\", str(text))\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[■◆●※▶▷▲→◇]', '', text)  # 불필요 기호 제거\n",
        "    lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > 10]\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "# 청킹 전략\n",
        "def split_chunks(text, max_length=500, overlap=100):\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    all_chunks = []\n",
        "\n",
        "    for para in paragraphs:\n",
        "        # 문장 단위 분리\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) <= max_length:\n",
        "                current_chunk += sentence + \" \"\n",
        "            else:\n",
        "                all_chunks.append(current_chunk.strip())\n",
        "                current_chunk = current_chunk[-overlap:] + sentence + \" \"\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            all_chunks.append(current_chunk.strip())\n",
        "\n",
        "    return [chunk for chunk in all_chunks if len(chunk) > 30]\n",
        "\n",
        "# ✅ 파일명 정규화 함수\n",
        "def normalize_filename(name: str) -> str:\n",
        "    # 1. NFC 정규화 (한글 자소 분리 방지)\n",
        "    name = unicodedata.normalize(\"NFC\", str(name))\n",
        "    # 2. 공백 양쪽 제거\n",
        "    name = name.strip()\n",
        "    # 3. 괄호, 따옴표, 하이픈 포함한 다양한 특수문자 제거\n",
        "    name = re.sub(r\"[\\\\/*?\\\"<>|‛‘’“”'「」【】·~!@#$%^&+=;,…ㆍ—−‒–―•※→←↑↓★☆♥♡•′″\\[\\]\\(\\)\\{\\},\\-]\", '', name)\n",
        "    return name\n",
        "\n",
        "# ✅ 유사 파일명을 통한 URL 매핑 함수\n",
        "def find_best_url(base_name_norm, url_mapping, cutoff=0.7):\n",
        "    # 유사도 100%\n",
        "    if base_name_norm in url_mapping:\n",
        "        return url_mapping[base_name_norm]\n",
        "\n",
        "    # 이후 파일명 부일치시 유사도 기반 매칭\n",
        "    # 모든 후보에 대해 유사도 계산\n",
        "    candidates = []\n",
        "    for key in url_mapping.keys():\n",
        "        sim = SequenceMatcher(None, base_name_norm, key).ratio()\n",
        "        candidates.append((key, sim))\n",
        "\n",
        "    # 가장 높은 유사도 항목 추출\n",
        "    best_match = max(candidates, key=lambda x: x[1])\n",
        "    match_name, best_score = best_match\n",
        "\n",
        "    if best_score >= cutoff:\n",
        "        print(f\"⚠️ 파일명 불일치 → 유사도 기반 매칭 사용: '{base_name_norm}' → '{match_name}' (유사도: {best_score:.2f})\")\n",
        "        return url_mapping[match_name]\n",
        "    else:\n",
        "        print(f\"⛔ 파일명 매칭 실패: '{base_name_norm}' → 가장 비슷한 항목: '{match_name}' (유사도: {best_score:.2f})\")\n",
        "        return \"출처 URL 없음\"\n",
        "\n",
        "# OCR 함수(사진 파일 .jpg, .jpeg, .png)\n",
        "def extract_text_from_image(image_path):\n",
        "    try:\n",
        "        from PIL import Image\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        image_np = np.array(img)[:, :, ::-1]  # RGB → BGR\n",
        "        result = ocr.ocr(image_np, cls=True)\n",
        "        text = \"\\n\".join([line[1][0] for line in result[0]])\n",
        "        if text.strip():\n",
        "            print(f\"🖼️ {os.path.basename(image_path)}: 이미지 OCR 추출 성공\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"❌ [이미지 OCR 실패] {image_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# OCR 함수(pdf에 텍스트말고 사진이 저장된 경우)\n",
        "def extract_text_with_paddleocr(pdf_path):\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=300, poppler_path=\"/usr/bin\")\n",
        "        full_text = \"\"\n",
        "        for i, page in enumerate(images):\n",
        "            image_np = np.array(page)[:, :, ::-1]  # RGB -> BGR\n",
        "            result = ocr.ocr(image_np, cls=True)\n",
        "            for line in result[0]:\n",
        "                full_text += line[1][0] + \"\\n\"\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        print(f\"❌ [OCR 실패] {os.path.basename(pdf_path)}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# ✅ PDF 텍스트 추출 함수 (PyPDF2 -> pdfplumber -> ocr 순으로 모든 pdf 파일 텍스트 추출)\n",
        "def read_file(filepath):\n",
        "    def try_pypdf2(path):\n",
        "        class SkipHandler(logging.Handler):\n",
        "            def emit(self, record):\n",
        "                if \"incorrect startxref pointer\" in record.getMessage():\n",
        "                    raise ValueError(\"skip_due_to_incorrect_startxref\")\n",
        "\n",
        "        handler = SkipHandler()\n",
        "        logger = logging.getLogger(\"PyPDF2._reader\")\n",
        "        logger.addHandler(handler)\n",
        "        logger.setLevel(logging.WARNING)\n",
        "\n",
        "        try:\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "                warnings.simplefilter(\"always\")\n",
        "                reader = PdfReader(path)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() or \"\"\n",
        "                for warning in w:\n",
        "                    msg = str(warning.message)\n",
        "                    if \"KSCms-UHC-H\" in msg:\n",
        "                        print(f\"⚠️ {os.path.basename(path)}: 한글 글꼴 미지원 (KSCms-UHC-H) → pdfplumber 시도\")\n",
        "                        return \"\"\n",
        "                return text\n",
        "        except ValueError as ve:\n",
        "            if \"skip_due_to_incorrect_startxref\" in str(ve):\n",
        "                print(f\"⚠️ {os.path.basename(path)}: 구조 손상 (startxref) → pdfplumber 시도\")\n",
        "                return \"\"\n",
        "            else:\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            print(f\"❌ [PyPDF2 실패] {path}: {e}\")\n",
        "            return \"\"\n",
        "        finally:\n",
        "            logger.removeHandler(handler)\n",
        "\n",
        "    def try_pdfplumber(path):\n",
        "        try:\n",
        "            import pdfplumber\n",
        "            with pdfplumber.open(path) as pdf:\n",
        "                text = \"\"\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                if text.strip():\n",
        "                    print(f\"📄 {os.path.basename(path)}: pdfplumber 추출 성공\")\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"❌ [pdfplumber 실패] {path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    if filepath.endswith(\".pdf\"):\n",
        "        text = try_pypdf2(filepath)\n",
        "        if not text.strip():\n",
        "            text = try_pdfplumber(filepath)\n",
        "        return text\n",
        "\n",
        "    elif filepath.endswith(\".md\"):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "    elif filepath.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        return extract_text_from_image(filepath)\n",
        "\n",
        "    elif filepath.lower().endswith(('.xlsx', '.xls')):\n",
        "        try:\n",
        "            xls = pd.read_excel(filepath, sheet_name=None, dtype=str)\n",
        "            all_text = \"\"\n",
        "            for sheet_name, df in xls.items():\n",
        "                df = df.fillna(\"\")\n",
        "                all_text += f\"\\n=== 시트: {sheet_name} ===\\n\"\n",
        "                all_text += \"\\n\".join([\"\\t\".join(row) for row in df.values.tolist()]) + \"\\n\"\n",
        "            return all_text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 엑셀 읽기 실패: {filepath} ({e})\")\n",
        "            return \"\"\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def get_all_files(folder):\n",
        "    all_files = []\n",
        "    for dirpath, _, filenames in os.walk(folder):\n",
        "        for f in filenames:\n",
        "            if f.endswith(('.pdf', '.md', '.jpg', '.jpeg', '.png', '.xlsx', '.xls')):\n",
        "                all_files.append(os.path.join(dirpath, f))\n",
        "    return all_files\n",
        "\n",
        "def extract_category(file_path, base_folder):\n",
        "    rel_path = os.path.relpath(file_path, start=base_folder)\n",
        "    parts = rel_path.split(os.sep)\n",
        "    return parts[0] if len(parts) > 1 else \"기타\"\n",
        "\n",
        "def build_metadata(file_path, base_folder, base_name, source_url, ext):\n",
        "    category = extract_category(file_path, base_folder)\n",
        "    department_path = os.path.relpath(base_folder, ROOT_FOLDER)\n",
        "\n",
        "    # 확장자 기반으로 source_type 분기\n",
        "    if ext == \".pdf\":\n",
        "        source_type = \"pdf\"\n",
        "    elif ext == \".md\":\n",
        "        source_type = \"markdown\"\n",
        "    elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "        source_type = \"image\"\n",
        "    elif ext in [\".xlsx\", \".xls\"]:\n",
        "        source_type = \"excel\"\n",
        "    else:\n",
        "        source_type = \"etc\"  # 혹시 모를 확장자에 대한 대비\n",
        "\n",
        "    return {\n",
        "        \"file_name\": base_name,\n",
        "        \"department\": department_path,\n",
        "        \"category\": category,\n",
        "        \"source_type\": source_type,\n",
        "        \"source_url\": source_url,\n",
        "        \"date\": datetime.now(KST).isoformat()\n",
        "    }\n",
        "\n",
        "total_added = 0\n",
        "failed_matches = []\n",
        "\n",
        "# 엑셀 기준으로 저장\n",
        "for dirpath, _, filenames in os.walk(ROOT_FOLDER):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith(URL_MAPPING_SUFFIX):\n",
        "            folder = dirpath  # 엑셀 위치 폴더 = 기준 폴더\n",
        "            dept_folder_name = os.path.basename(folder)\n",
        "            url_file = os.path.join(folder, filename)\n",
        "\n",
        "            try:\n",
        "                url_df = pd.read_excel(url_file)\n",
        "                url_df.columns = url_df.columns.str.strip()\n",
        "                url_df['파일명_정규화'] = url_df['파일명'].apply(lambda x: normalize_filename(os.path.splitext(str(x))[0]))\n",
        "                url_df['URL'] = url_df['URL'].astype(str).str.strip()\n",
        "                url_mapping = dict(zip(url_df['파일명_정규화'], url_df['URL']))\n",
        "            except Exception as e:\n",
        "                print(f\"❌ URL 매핑 실패: {url_file} ({e})\")\n",
        "                continue\n",
        "\n",
        "            file_paths = get_all_files(folder)\n",
        "\n",
        "            for file_path in file_paths:\n",
        "                file_name = os.path.basename(file_path)\n",
        "                base_name = os.path.splitext(file_name)[0]\n",
        "                ext = os.path.splitext(file_name)[1].lower()\n",
        "\n",
        "                if file_name.endswith(URL_MAPPING_SUFFIX):\n",
        "                    continue\n",
        "\n",
        "                # 텍스트 추출 결과가 공백이거나 빈 문자열일 경우 해당 pdf 파일은 사진으로 구성된 경우임\n",
        "                raw_text = read_file(file_path)\n",
        "                if not raw_text.strip():\n",
        "                    print(f\"⚠️ {file_name}: 텍스트 없음 → PaddleOCR 시도\")\n",
        "                    raw_text = extract_text_with_paddleocr(file_path)\n",
        "\n",
        "                # 그래도 텍스트 없으면 최종 스킵\n",
        "                if not raw_text.strip():\n",
        "                    print(f\"⛔ {file_name}: PyPDF2, pdfplumber, PaddleOCR 방법으로 텍스트 추출 실패 → 스킵\")\n",
        "                    continue\n",
        "\n",
        "                # ✅ URL 매핑 실패하면 바로 전체 스킵\n",
        "                norm_base = normalize_filename(base_name)\n",
        "                source_url = find_best_url(norm_base, url_mapping)\n",
        "                if source_url == \"출처 URL 없음\":\n",
        "                    failed_matches.append((base_name, dept_folder_name))\n",
        "                    print(f\"⛔ {file_name}: URL 매칭 실패 (→ {norm_base}) → 벡터 저장 스킵\")\n",
        "                    continue\n",
        "\n",
        "                cleaned = clean_text(raw_text) # 전처리 전략 함수 호출\n",
        "                chunks = split_chunks(cleaned, max_length=500, overlap=100) # 청킹 전략 함수 호출\n",
        "                embeddings = model.encode(chunks).tolist()\n",
        "                ids = [f\"{dept_folder_name}_{base_name}_chunk_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "                new_chunks, new_embeddings, new_ids, new_metadatas = [], [], [], []\n",
        "\n",
        "                for chunk, emb, id_ in zip(chunks, embeddings, ids):\n",
        "                    if id_ not in existing_ids:\n",
        "                        meta = build_metadata(file_path, folder, base_name, source_url, ext)\n",
        "                        new_chunks.append(chunk)\n",
        "                        new_embeddings.append(emb)\n",
        "                        new_ids.append(id_)\n",
        "                        new_metadatas.append(meta)\n",
        "\n",
        "                if new_chunks:\n",
        "                    collection.add(\n",
        "                        documents=new_chunks,\n",
        "                        embeddings=new_embeddings,\n",
        "                        metadatas=new_metadatas,\n",
        "                        ids=new_ids\n",
        "                    )\n",
        "                    print(f\"✅ {file_name}: {len(new_chunks)}개 저장 완료\")\n",
        "                    total_added += len(new_chunks)\n",
        "                else:\n",
        "                    print(f\"🟦 {file_name}: 중복 청크 존재 → 스킵\")\n",
        "\n",
        "\n",
        "if failed_matches:\n",
        "    print(f\"\\nURL 매핑 실패 파일 수: {len(failed_matches)}\")\n",
        "    for base_name, dept in failed_matches:\n",
        "        print(f\"📂 [{dept}] - {base_name}\")\n",
        "else:\n",
        "    print(\"\\n✅ 모든 파일이 URL 매핑에 성공했습니다.\")\n",
        "\n",
        "print(f\"\\n🎉 총 저장된 신규 청크 수: {total_added}\")"
      ],
      "metadata": {
        "id": "YNaggdr8dqHb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "# 설정\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/chroma_index\"\n",
        "COLLECTION_NAME = \"knu_cheonan_collection\"\n",
        "\n",
        "client = PersistentClient(path=PERSIST_DIR)\n",
        "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "\n",
        "# 전체 문서 불러오기\n",
        "results = collection.get(limit=None, include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
        "total_docs = len(results['ids'])\n",
        "\n",
        "print(f\"\\n📦 전체 저장된 벡터 수: {total_docs}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 10개 무작위 추출\n",
        "sample_indices = random.sample(range(total_docs), min(10, total_docs))\n",
        "\n",
        "for i in sample_indices:\n",
        "    doc_id = results[\"ids\"][i]\n",
        "    metadata = results[\"metadatas\"][i]\n",
        "    doc_text = results[\"documents\"][i].strip().replace(\"\\n\", \" \")\n",
        "    embedding = results[\"embeddings\"][i]\n",
        "\n",
        "    print(f\"🔹 ID: {doc_id}\")\n",
        "    print(f\"   📁 파일명: {metadata.get('file_name')}\")\n",
        "    print(f\"   🏫 부서: {metadata.get('department')}\")\n",
        "    print(f\"   📂 카테고리: {metadata.get('category')}\")\n",
        "    print(f\"   📄 타입: {metadata.get('source_type')}\")\n",
        "    print(f\"   🌐 URL: {metadata.get('source_url')}\")\n",
        "    print(f\"   📅 날짜: {metadata.get('date')[:10]}\")\n",
        "    print(f\"   💬 청크 문장: {doc_text[:200]}...\")  # 200자까지만 표시\n",
        "    print(f\"   🧠 벡터 길이: {len(embedding)}\")\n",
        "    print(f\"   🔢 벡터 앞 5개 값: {embedding[:5]}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "F0gqtPA8wvRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089b1b44-1e79-43da-c9e7-4b83aeb153b1",
        "collapsed": true
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 전체 저장된 벡터 수: 1095\n",
            "======================================================================\n",
            "🔹 ID: Software Department (소프트웨어학과)_공결 규정입니다._chunk_0\n",
            "   📁 파일명: 공결 규정입니다.\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (규정자료실)\n",
            "   📄 타입: markdown\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1426/293859/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: # 공결 규정입니다. **출처:** [https://sw.kongju.ac.kr/bbs/ZD1180/1426/293859/artclView.do](https://sw.kongju.ac.kr/bbs/ZD1180/1426/293859/artclView.do) **작성자:** 임성철 **작성일:** 2024.03.07 ## 본문 <공결 및 병결 관련 국립공주대학...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [ 0.04841236  0.03040045  0.10784128  0.02828371 -0.08069456]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_2022학년도 동계방학 「한국조폐공사」 오픈캠퍼스 현장실습 지원자 모집_chunk_3\n",
            "   📁 파일명: 2022학년도 동계방학 「한국조폐공사」 오픈캠퍼스 현장실습 지원자 모집\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: markdown\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/261929/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: 하며, 소속 학과에 현장실습 교과목이 개설되는 경우 전공과목으로 수강 가능 붙임 1. 현장실습 운영계획서 1부. 2. [학생 제출 서식] 신청서 1부. 3. 관련공문 1부. 끝. ## 첨부파일 - [https://sw.kongju.ac.kr/bbs/ZD1180/1423/471463/download.do](https://sw.kongju.ac.kr/bbs/Z...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [ 0.07321277  0.08540119  0.06262037  0.06024772 -0.04298493]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_성적우수장학금 증빙파일 공유 안내_chunk_1\n",
            "   📁 파일명: 성적우수장학금 증빙파일 공유 안내\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: markdown\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/249654/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: 바랍니다 . 성적우수장학금 제출기한 : 2.6 (일) 공통적으로 네이버 클라우드에 bitnine@naver.com 에게 \"2021 학년 2 학기 성적우수 \" 폴더를 공유합니다 . 현시간부로 기존 공유 되었던 자료를 모두 해제 하였으니 다시 공유신청 바랍니다 폴더구조 학번 _이름 <- 최상위 폴더이며 200901234_홍길동 과 같이 생성 - 성적우수장학금 ...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [-0.02348396 -0.00345887 -0.00317589  0.02142402 -0.05858001]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_(필독)2021-1 수강신청 안내문(컴퓨터공학전공)_chunk_14\n",
            "   📁 파일명: (필독)2021-1 수강신청 안내문(컴퓨터공학전공)\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: pdf\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/208894/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: 월 이내에 졸업논문 계획서 [별지서식 (1)]을 전공주임에게 제출하여 승인을 받아야 한다. ② 제출한 졸업논문 계획서를 변경하고자 할 때에는 소속 전공주임의 승인을 얻어야 한다. ③ 단독연구를 원칙으로 하되, 지도교수의 승인 하에 3인 이내의 공동연구를 허용할 수 있다. ④ 지도교수는 졸업논문의 자료준비 , 작성 및 발표 등에 관한 사항을 지도한다 . 제5...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [ 0.04314197  0.07849857  0.07001641  0.04382179 -0.01814381]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_1학년 직업흥미검사 안내_chunk_3\n",
            "   📁 파일명: 1학년 직업흥미검사 안내\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (특강공지)\n",
            "   📄 타입: pdf\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1424/208934/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: 직업에 적합한지 , 어떤 환경이 그 개인에게 적합한지 , 어떤 사람들과 일하는 것을 좋아하는지 등에 관한 정보를 제시하는 척도별 점수(GOT, BIS, PSS)가 산출됩니다 . ○ STRONG 직업흥미검사의 이론적 근거 Lowman(1991) 은 “자신의 직업과 조직에 잘 맞는 사람들은 그렇지 않은 사람들보다 그 일에 대한 만족 도가 더 높고, 직업을 계속...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [-0.01110026  0.0588187  -0.01232119  0.01703704 -0.04225011]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_코로나19 대응 안내_chunk_1\n",
            "   📁 파일명: 코로나19 대응 안내\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: markdown\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/251794/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: 과 홈페이지 팝업에 있는 설문에 신속히 응답 학과 보고 ( http://naver.me/FIpapnll ) 공과대학 보고 ( http://naver.me/x78lfGvQ ) 2. 공결처리 안내 - 본인확진 : 위 1번의 2개의 설문에 응답 -> 치료 잘 하기 -> 격리해제후 등교시 붙임 파일의 공결신청서 & 증빙 학과사무실(8공 801호)제출 - 밀접접촉(...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [ 0.01281242  0.02225454  0.02842411  0.01811122 -0.04218608]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_[전학생] 2024학년도 1학기 추가수강 안내_chunk_1\n",
            "   📁 파일명: [전학생] 2024학년도 1학기 추가수강 안내\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: markdown\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/293370/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: .02.29 ## 본문 2024학년도 1학기 추가수강 안내 아래 교과목에 대해서 학과 자체적으로 추가수강 접수를 받으니 추가수강을 하고자 하는 학생은 해당 일시에 신청 바랍니다. - 리스트에 있는 강좌에 대해서는 교수님 찾아가고 연락 해도 소용 업음, 학과에서만 받음 - 그 외 과목은 \"수강정원 초과 교과목 추가 수강 확인서(신서식)\"를 작성(담당교수 날인...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [0.01500241 0.0320567  0.03864136 0.01245426 0.00263759]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_신입생 수강과목 목록_chunk_0\n",
            "   📁 파일명: 신입생 수강과목 목록\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: excel\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/208825/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: === 시트: Export === 1001019 01 원어민실용영어Ⅰ 천안캠퍼스 교필 1 2 2 0 월1 월2 2공404 20 택1 02 원어민실용영어Ⅰ 천안캠퍼스 교필 1 2 2 0 월3 월4 2공404 20 03 원어민실용영어Ⅰ 천안캠퍼스 교필 1 2 2 0 월5 월6 2공404 20 04 원어민실용영어Ⅰ 천안캠퍼스 교필 1 2 2 0 월7 월8 2공4...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [ 0.00383119  0.11121961  0.02452121 -0.11931653 -0.07199366]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_SWAI사업단 2022-동계 인턴십 프로그램 실습생 모집 공고_chunk_0\n",
            "   📁 파일명: SWAI사업단 2022-동계 인턴십 프로그램 실습생 모집 공고\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: markdown\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/263272/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: # SW/AI사업단 2022-동계 인턴십 프로그램 실습생 모집 공고 **출처:** [https://sw.kongju.ac.kr/bbs/ZD1180/1423/263272/artclView.do](https://sw.kongju.ac.kr/bbs/ZD1180/1423/263272/artclView.do) **작성자:** 임성철 **작성일:** 2022.11.2...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [-0.04881673  0.03529529  0.09625652  0.00971952 -0.0267488 ]\n",
            "----------------------------------------------------------------------\n",
            "🔹 ID: Software Department (소프트웨어학과)_2023 신입생 안내교육(OT) 자료_chunk_2\n",
            "   📁 파일명: 2023 신입생 안내교육(OT) 자료\n",
            "   🏫 부서: Kongju National University Cheonan Campus (공주대학교 천안캠퍼스)/Cheonan Campus Department (천안캠퍼스 학과별 사이트)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)\n",
            "   📂 카테고리: 커뮤니티 (학과공지)\n",
            "   📄 타입: pdf\n",
            "   🌐 URL: https://sw.kongju.ac.kr/bbs/ZD1180/1423/269342/artclView.do\n",
            "   📅 날짜: 2025-04-13\n",
            "   💬 청크 문장: 교육과정 -교육목표를달성하기위하여 , 그내용을체계적으로 조직한교육계획의전체 ▪수강신청 -강의나실험∙실습을받기위하여자신이이수할교과목을선택하여신청하는것 2. 수강신청 관련용어정의3. 교육과정의 구성 구분 학점 비고 교양필수기초교양필수 8공주대학교 학생들이공통으로수강해야하는교양과목 교양선택기초교양선택 14소프트웨어학과 학생들이수강해야교양과목 균형교양선택 12공과...\n",
            "   🧠 벡터 길이: 384\n",
            "   🔢 벡터 앞 5개 값: [ 0.03407622  0.02518597  0.01949668 -0.04839326 -0.00861201]\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from chromadb import PersistentClient\n",
        "import unicodedata\n",
        "\n",
        "# 1. ChromaDB 클라이언트 설정\n",
        "client = PersistentClient(path=\"/content/drive/MyDrive/chroma_index\")\n",
        "collection = client.get_collection(\"knu_cheonan_collection\")\n",
        "\n",
        "# 2. 모든 문서 및 메타데이터 가져오기 (ids 포함)\n",
        "all_data = collection.get(include=[\"documents\", \"metadatas\"])\n",
        "\n",
        "print(\"🔍 '신입생' 포함된 file_name 검색 중...\\n\")\n",
        "\n",
        "found = False\n",
        "\n",
        "# 3. 순회하면서 '신입생' 포함 여부 확인\n",
        "for i, meta in enumerate(all_data[\"metadatas\"]):\n",
        "    file_name = meta.get(\"file_name\", \"\")\n",
        "    normalized_name = unicodedata.normalize(\"NFC\", file_name)\n",
        "\n",
        "    if \"신입생 O.T\" in normalized_name:\n",
        "        found = True\n",
        "        print(f\"[{i+1}] ✅ ID: {all_data['ids'][i]}\")\n",
        "        print(f\"    📄 file_name: {repr(file_name)}\")\n",
        "        print(f\"    📎 metadata: {meta}\")\n",
        "        print(f\"    📝 문서 일부: {all_data['documents'][i][:80]}...\\n\")\n",
        "\n",
        "if not found:\n",
        "    print(\"❌ '신입생'이라는 단어가 포함된 file_name 메타데이터를 찾을 수 없습니다.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "i01w7CWKNS0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# 출처 URL 없는거 검색\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "# ✅ 1. ChromaDB 컬렉션 열기\n",
        "client = PersistentClient(path=\"/content/drive/MyDrive/chroma_index\")\n",
        "collection = client.get_collection(name=\"knu_cheonan_collection\")\n",
        "\n",
        "# ✅ 2. 전체 문서 조회\n",
        "results = collection.get(limit=None, include=[\"documents\", \"metadatas\"])\n",
        "\n",
        "# ✅ 3. '출처 URL 없음' 필터링\n",
        "no_url_chunks = []\n",
        "for id_, doc, meta in zip(results[\"ids\"], results[\"documents\"], results[\"metadatas\"]):\n",
        "    if meta.get(\"source_url\") == \"출처 URL 없음\":\n",
        "        no_url_chunks.append((id_, doc, meta))\n",
        "\n",
        "# ✅ 4. 결과 출력\n",
        "print(f\"\\n🔍 '출처 URL 없음' 청크 개수: {len(no_url_chunks)}\")\n",
        "\n",
        "for id_, doc, meta in no_url_chunks:\n",
        "    print(\"\\n🆔 ID:\", id_)\n",
        "    print(\"📝 청크 내용:\")\n",
        "    print(doc)\n",
        "    print(\"📎 메타데이터:\")\n",
        "    print(meta)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Miha3PQUDXFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 완전 내부 초기화 ------ 신중히 사용하기 바람 -------\n",
        "\"\"\"\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/chroma_index\"\n",
        "COLLECTION_NAME = \"knu_cheonan_collection\"\n",
        "\n",
        "client = PersistentClient(path=PERSIST_DIR)\n",
        "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "\n",
        "# 컬렉션 내의 모든 문서 ID를 가져와 삭제\n",
        "existing_ids = collection.get(limit=None)['ids']\n",
        "if existing_ids:\n",
        "    collection.delete(ids=existing_ids)\n",
        "    print(f\"🗑️ 컬렉션 내 모든 데이터 삭제 완료: {len(existing_ids)}개 삭제됨\")\n",
        "else:\n",
        "    print(\"✅ 삭제할 데이터가 없습니다. 컬렉션이 이미 비어있습니다.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "z8uhQAFgwRdd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# 컬렉션 Drop\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/chroma_index\"\n",
        "COLLECTION_NAME = \"knu_cheonan_collection\"\n",
        "\n",
        "client = PersistentClient(path=PERSIST_DIR)\n",
        "\n",
        "# 컬렉션 삭제\n",
        "client.delete_collection(name=COLLECTION_NAME)\n",
        "print(f\"🗑️ 컬렉션 '{COLLECTION_NAME}' 삭제 완료\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Rtb_DL890ylq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" PDF 텍스트 추출 성능 비교 => PyPDF2가 더 잘 나옴\n",
        "!pip install PyPDF2\n",
        "!pip install pdfplumber\n",
        "\n",
        "import PyPDF2\n",
        "import pdfplumber\n",
        "\n",
        "with open(\"/content/drive/MyDrive/document_files/Department (학부)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)/커뮤니티(규정자료실)/미래설계 교과목 이수기준(2021.04.19.).pdf\", \"rb\") as f:\n",
        "    reader = PyPDF2.PdfReader(f)\n",
        "    print(\"[PyPDF2]\")\n",
        "    print(reader.pages[0].extract_text())\n",
        "\n",
        "with pdfplumber.open(\"/content/drive/MyDrive/document_files/Department (학부)/Cheonan College of Engineering (천안공과대학)/Software Department (소프트웨어학과)/커뮤니티(규정자료실)/미래설계 교과목 이수기준(2021.04.19.).pdf\") as pdf:\n",
        "    print(\"\\n[pdfplumber]\")\n",
        "    print(pdf.pages[0].extract_text())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EYw4poMgwzUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 예제 코드에서 사용할 OCR 설치\n",
        "# Tesseract OCR => 추출이 이상하게 됨\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract pdf2image Pillow\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" 이거 사용함\n",
        "# PaddleOCR 설치 (최초 1회) => 한글 지원 매우 좋고, 정확도도 높음, 딥러닝 기반 고정밀 OCR\n",
        "!pip install paddleocr\n",
        "!pip install \"paddlepaddle==2.6.2\" -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install matplotlib\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "iJ-_yVBdZecg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# 이미지 경로\n",
        "IMG_PATH = \"/content/drive/MyDrive/sample/공결규정_사본.jpg\"\n",
        "\n",
        "# 이미지 로드 (전처리 없음)\n",
        "img = Image.open(IMG_PATH)\n",
        "\n",
        "# OCR 수행\n",
        "text = pytesseract.image_to_string(img, lang=\"kor+eng\", config=\"--psm 6 --oem 3\")\n",
        "\n",
        "# 결과 출력\n",
        "print(\"🖼️ 이미지 OCR 추출 결과:\\n\")\n",
        "print(text)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tDEsQAsIVIoY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "# 설정\n",
        "POPPLER_PATH = \"/usr/bin\"\n",
        "PDF_PATH = \"/content/drive/MyDrive/sample/2. 홍보자료_ICT 융합인재 양성과정.pdf\"\n",
        "LANG = \"kor+eng\"\n",
        "\n",
        "# PDF → 이미지 변환\n",
        "images = convert_from_path(PDF_PATH, dpi=300, poppler_path=POPPLER_PATH)\n",
        "\n",
        "# OCR 실행, 이미지 전처리 없어 기본 모델 성능만으로 텍스트 추출\n",
        "for i, page in enumerate(images):\n",
        "    text = pytesseract.image_to_string(page, lang=LANG)\n",
        "    print(f\"\\n--- Page {i+1} ---\")\n",
        "    print(text)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SkVvFS7tVANY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from paddleocr import PaddleOCR\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# OCR 초기화 (한글 지원)\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='korean')\n",
        "\n",
        "# 이미지 경로\n",
        "IMG_PATH = \"/content/drive/MyDrive/sample/공결규정_사본.jpg\"\n",
        "\n",
        "# 이미지 로드 (전처리 없음)\n",
        "img = Image.open(IMG_PATH)\n",
        "image_np = np.array(img.convert(\"RGB\"))[:, :, ::-1]  # PIL → NumPy(BGR)\n",
        "\n",
        "# OCR 수행\n",
        "result = ocr.ocr(image_np, cls=True)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"🖼️ 이미지 OCR 추출 결과:\\n\")\n",
        "for line in result[0]:\n",
        "    text = line[1][0]\n",
        "    print(text)\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "6CmuGaIqiiBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from paddleocr import PaddleOCR\n",
        "from pdf2image import convert_from_path\n",
        "import numpy as np\n",
        "\n",
        "# 경로 설정\n",
        "POPPLER_PATH = \"/usr/bin\"\n",
        "PDF_PATH = \"/content/drive/MyDrive/sample/2. 홍보자료_ICT 융합인재 양성과정.pdf\"\n",
        "\n",
        "# OCR 초기화 (한글 지원)\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='korean')\n",
        "\n",
        "# PDF → 이미지 변환\n",
        "images = convert_from_path(PDF_PATH, dpi=300, poppler_path=POPPLER_PATH)\n",
        "\n",
        "# OCR 실행, 이미지 전처리 없어 기본 모델 성능만으로 텍스트 추출\n",
        "full_text = \"\"\n",
        "for i, page in enumerate(images):\n",
        "    image_np = np.array(page)[:, :, ::-1]  # PIL → numpy (BGR)\n",
        "\n",
        "    result = ocr.ocr(image_np, cls=True)\n",
        "\n",
        "    full_text += f\"\\n--- Page {i+1} ---\\n\"\n",
        "    for line in result[0]:\n",
        "        full_text += line[1][0] + \"\\n\"\n",
        "\n",
        "# 결과 출력\n",
        "print(\"📄 전체 OCR 텍스트 결과:\\n\")\n",
        "print(full_text)\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "klIARJL-ayNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# 엑셀파일 -> 텍스트 추출\n",
        "!pip install openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ 파일 경로 설정 (입력한 그대로 사용)\n",
        "PDF_PATH = \"/content/drive/MyDrive/sample/Software Department (소프트웨어학과).xlsx\"\n",
        "\n",
        "def extract_text_from_excel(file_path):\n",
        "    try:\n",
        "        xls = pd.read_excel(file_path, sheet_name=None, dtype=str)\n",
        "        all_text = \"\"\n",
        "\n",
        "        for sheet_name, df in xls.items():\n",
        "            df = df.fillna(\"\")\n",
        "            sheet_text = \"\\n\".join([\"\\t\".join(row) for row in df.values.tolist()])\n",
        "            all_text += f\"\\n=== 시트: {sheet_name} ===\\n{sheet_text}\\n\"\n",
        "\n",
        "        return all_text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 엑셀 읽기 실패: {file_path} ({e})\")\n",
        "        return \"\"\n",
        "\n",
        "# ✅ 텍스트 추출 실행\n",
        "extracted_text = extract_text_from_excel(PDF_PATH)\n",
        "\n",
        "if extracted_text:\n",
        "    print(\"✅ 텍스트 추출 완료 (앞부분 미리보기):\\n\")\n",
        "    print(extracted_text[:3000])  # 너무 길 경우 앞부분만 미리보기\n",
        "else:\n",
        "    print(\"❌ 텍스트 추출 실패 또는 내용 없음.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "YlrEKAw9nV4K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}