{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KNUckle-llm/experiments/blob/main/Q%26A_ChatBot2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEER2kRiIwvS"
      },
      "outputs": [],
      "source": [
        "# ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡ í”„ë¡œì íŠ¸ 2-4\n",
        "# ì¶”ê°€ì  1 : Goole Vision ì‚¬ìš©í•´ì„œ pdfì¸ë° ì‚¬ì§„ìœ¼ë¡œ ëœ ê²ƒë“¤ ì €ì¥í•˜ê¸°(ì™„)\n",
        "# ì¶”ê°€ì  2 : ê° ë¶€ì„œë³„ì— í•´ë‹¹í•˜ëŠ” ë²¡í„° DBë¥¼ ê°€ì ¸ì™€ retrieverë¡œ ê²€ìƒ‰í•˜ëŠ” ë°©ì‹, ì„ íƒ ì•ˆí•˜ë©´ ëª¨ë“  ë¶€ì„œ ì •ë³´ê°€ ë“¤ì–´ê°„ DBë¥¼ ê°€ì ¸ì™€ ê²€ìƒ‰(ì™„)\n",
        "# ì¶”ê°€ì  3 : ê° ë¶€ì„œë³„ë¡œ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ë‹¤ë¥¸ê±° ì‚¬ìš©í•˜ê¸°(ë¯¸ì™„)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s1WwBRU_Nq9y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5KCM5kP6No8S"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3R-NCn3dOPWk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_openai==0.3.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Mw_Wf-zPhtF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface==0.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hF1lOXXOPkQX"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community==0.3.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DW2y0rt0Rjjv"
      },
      "outputs": [],
      "source": [
        "#!pip install faiss-cpu==1.10.0\n",
        "!pip install langchain_chroma==0.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D5quX3Ac8eVU"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RG_3magVch41"
      },
      "outputs": [],
      "source": [
        "#!pip install pydantic==2.10.6\n",
        "#!pip uninstall -y gradio\n",
        "#!pip install --upgrade gradio gradio-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qXZuuZ05P3qO"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall numpy -y\n",
        "#!pip install --no-cache-dir numpy==1.26.4\n",
        "# colabì— numpy 2.ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì„œ ë²„ì „ ì¶©ëŒë‚¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqCQ32ozSclI",
        "outputId": "e7d7c9cb-a737-4e6b-e563-89a55a582df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObqvRExvFmXG",
        "outputId": "436ddcd0-e3dd-45ad-8ac1-8599866e5431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.31.0\n"
          ]
        }
      ],
      "source": [
        "import gradio\n",
        "print(gradio.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MXM0w5Q5t0c"
      },
      "outputs": [],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-vision pdf2image\n",
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "id": "Pb1yms9cVH0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krtuPwiOIwfj"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import uuid\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°(openai API í‚¤)\n",
        "load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')\n",
        "\n",
        "# LLM ì„¤ì •\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë¶„ë¦¬\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "system_message = \"\"\"\n",
        "ë‹¹ì‹ ì€ ê³µì£¼ëŒ€í•™êµì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
        "ê³µì‹ ë¬¸ì„œë‚˜ ê³µì£¼ëŒ€í•™êµ ì‚¬ì´íŠ¸ì—ì„œ ì œê³µë˜ëŠ” ì •ë³´ë§Œ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n",
        "ë¬¸ë§¥ì—ì„œ ëª…í™•í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ \"ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë§í•´ì£¼ì„¸ìš”.\n",
        "ì •í™•í•œ ì¶œì²˜ë¥¼ ì•„ë˜ì™€ ê°™ì´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.\n",
        "íŒŒì¼ëª… :\n",
        "ë¶€ì„œ/í•™ê³¼ :\n",
        "URL :\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_message),\n",
        "        (\"placeholder\", \"{memory}\"),\n",
        "        (\"user\", \"ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:\\n{context}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ì¶œë ¥ íŒŒì„œ\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# íŠ¸ë¦¬ë¨¸ ì„¤ì •\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=500,\n",
        "    token_counter=llm,\n",
        "    strategy=\"last\",\n",
        "    include_system=True,\n",
        "    start_on=\"human\"\n",
        ")\n",
        "\n",
        "DEPARTMENT_MAP = {\n",
        "    \"ALL(ì „ì²´)\" : \"knu_chroma_db_all\",\n",
        "    \"Software Department (ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼)\": \"knu_chroma_db_software\",\n",
        "    \"Department of Computer Engineering (ì»´í“¨í„°ê³µí•™ê³¼)\": \"knu_chroma_db_computer\",\n",
        "    #\"ë™ì•„ë¦¬\": \"knu_chroma_db_club\",\n",
        "    #\"ëŒ€í•™ì›\": \"knu_chroma_db_gradschool\",\n",
        "    #\"ì†Œí”„íŠ¸ì›¨ì–´ì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\": \"knu_chroma_db_center\"\n",
        "}\n",
        "\n",
        "# ì „ì—­\n",
        "retriever = None\n",
        "rag_chain = None\n",
        "history_store: dict[str, InMemoryChatMessageHistory] = {}\n",
        "\n",
        "# ì„¸ì…˜ ì´ˆê¸°í™” í•¨ìˆ˜\n",
        "def init_session(session_id: str):\n",
        "    if not session_id or not isinstance(session_id, str):\n",
        "        raise ValueError(\"ìœ íš¨í•œ session_idê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "    if session_id not in history_store:\n",
        "        history_store[session_id] = InMemoryChatMessageHistory()\n",
        "\n",
        "    # SystemMessageê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "    existing = history_store[session_id].messages\n",
        "    if not any(msg.type == \"system\" and msg.content.strip() == system_message.strip() for msg in existing):\n",
        "        history_store[session_id].add_message(SystemMessage(content=system_message.strip()))\n",
        "\n",
        "# ì„¸ì…˜ íˆìŠ¤í† ë¦¬ í•¨ìˆ˜\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    return history_store[session_id]\n",
        "\n",
        "def load_all_docs_once():\n",
        "    global all_docs\n",
        "    if not all_docs:\n",
        "        for dept, dir_name in DEPARTMENT_MAP.items():\n",
        "            db = Chroma(persist_directory=f\"/content/drive/MyDrive/ChromaDB/{dir_name}\", embedding_function=hf_embeddings)\n",
        "            res = db.get(include=[\"documents\", \"metadatas\"])\n",
        "            all_docs.extend([Document(page_content=d, metadata=m) for d, m in zip(res[\"documents\"], res[\"metadatas\"])])\n",
        "\n",
        "# RAG ì²´ì¸ ë¡œë“œ\n",
        "def rebuild_chain(selected_dept: str = None):\n",
        "    global retriever, rag_chain\n",
        "\n",
        "    # ì‚¬ìš©ìê°€ ë¶€ì„œë¥¼ ì„ íƒí•œ ê²½ìš°: í•´ë‹¹ ë¶€ì„œ ì „ìš© DBë§Œ ë¶ˆëŸ¬ì™€ retriever êµ¬ì„±(ë¹ ë¦„)\n",
        "    if selected_dept:\n",
        "        db_path = f\"/content/drive/MyDrive/ChromaDB/{DEPARTMENT_MAP[selected_dept]}\"\n",
        "        db = Chroma(persist_directory=db_path, embedding_function=hf_embeddings)\n",
        "    else:\n",
        "        # ë¶€ì„œë¥¼ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš°: ëª¨ë“  ë¶€ì„œì˜ ë¬¸ì„œê°€ ì €ì¥ëœ DBë¥¼ ë¡œë“œí•˜ì—¬ retriever êµ¬ì„±(ëŠë¦¼)\n",
        "        db_path = f\"/content/drive/MyDrive/ChromaDB/knu_chroma_db_all\"\n",
        "        db = Chroma(persist_directory=db_path, embedding_function=hf_embeddings)\n",
        "\n",
        "    \"\"\"\n",
        "    ë²¡í„° ê±°ë¦¬ ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰\n",
        "    ì§ˆë¬¸ -> MultiQuery(3ê°œì˜ ì„œë¸Œ ì§ˆë¬¸)\n",
        "    ì„œë¸Œ ì§ˆë¬¸ 1 -> ChromaDBì—ì„œ mmrë°©ì‹(ë‹¤ì–‘í•˜ê³ ë„ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë°©ì‹)ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 10ê°œ í›„ë³´ ì¤‘ 5ê°œë¥¼ ì„ íƒ\n",
        "    ì„œë¸Œ ì§ˆë¬¸ 2 -> ChromaDBì—ì„œ mmrë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 10ê°œ í›„ë³´ ì¤‘ 5ê°œë¥¼ ì„ íƒ\n",
        "    ì„œë¸Œ ì§ˆë¬¸ 3 -> ChromaDBì—ì„œ mmrë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 10ê°œ í›„ë³´ ì¤‘ 5ê°œë¥¼ ì„ íƒ\n",
        "    -> ì „ì²´ 15ê°œ ì²­í¬ ê²€ìƒ‰ë¨ -> ì¤‘ë³µ ì œê±° -> ê³ ìœ  ì²­í¬ Nê°œ(15ê°œ ì´í•˜)\n",
        "\n",
        "    í‚¤ì›Œë“œ ê¸°ë°˜ ì¬ì •ë ¬ - Document ë¦¬ìŠ¤íŠ¸ ìƒíƒœì—ì„œ ì‘ë™í•¨\n",
        "    ì²­í¬ Nê°œë¥¼ ëŒ€ìƒìœ¼ë¡œ BM25(í‚¤ì›Œë“œ(=ì§ˆë¬¸ì—ì„œ ë‚˜ì˜¨ ë‹¨ì–´)ê°€ ë¬¸ì„œ ì•ˆì— ì–¼ë§ˆë‚˜ ì˜ ë“±ì¥í•˜ëŠ”ì§€ ê´€ë ¨ì„±ì„ ìˆ˜ì¹˜í™”í•˜ëŠ” ë°©ì‹)\n",
        "    -> top 3ê°œ ì„ íƒ\n",
        "    -> LLM contextì— top3 ì²­í¬ 3ê°œ ì‚½ì… -> ë‹µë³€ ìƒì„±\n",
        "    \"\"\"\n",
        "    # ì „ì²´ ë¬¸ì„œ ë¡œë”© í›„ BM25 êµ¬ì„±ìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ í™•ë³´\n",
        "    results = db.get(include=[\"documents\", \"metadatas\"])\n",
        "    docs = [Document(page_content=doc, metadata=meta) for doc, meta in zip(results[\"documents\"], results[\"metadatas\"])]\n",
        "\n",
        "    # Chroma + BM25 í˜¼í•© ê²€ìƒ‰ê¸° êµ¬ì„±\n",
        "    chroma_retriever = db.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"k\": 3, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
        "    )\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(docs)\n",
        "    bm25_retriever.k = 3\n",
        "    base_retriever = EnsembleRetriever(retrievers=[chroma_retriever, bm25_retriever])\n",
        "    retriever = MultiQueryRetriever.from_llm(base_retriever, llm=llm)\n",
        "\n",
        "    # ìµœì¢… RAG ì²´ì¸\n",
        "    rag_chain_core = {\n",
        "        \"memory\": trimmer,      # trimmer ì ìš©\n",
        "        \"context\": retriever,\n",
        "        \"input\": RunnablePassthrough()\n",
        "    } | prompt_template | llm | parser\n",
        "\n",
        "    # ìµœì¢… RAG ì²´ì¸\n",
        "    rag_chain = RunnableWithMessageHistory(\n",
        "        rag_chain_core,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"memory\"\n",
        "    )\n",
        "\n",
        "# ì§ˆë¬¸ ì²˜ë¦¬\n",
        "def answer_question(question: str, session_id: str) -> str:\n",
        "    if not session_id:\n",
        "        return \"âš ï¸ ì„¸ì…˜ì„ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "    init_session(session_id)\n",
        "\n",
        "    # rag_chainì˜ ì…ë ¥ í˜•ì‹ì— ë§ì¶° dict êµ¬ì„±\n",
        "    chain_input = {\n",
        "        \"memory\": None,        # RunnableWithMessageHistoryê°€ ì•Œì•„ì„œ ì²˜ë¦¬\n",
        "        \"context\": None,       # retrieverê°€ ì•Œì•„ì„œ ì²˜ë¦¬\n",
        "        \"input\": question\n",
        "    }\n",
        "\n",
        "    # RAG ì²´ì¸ í˜¸ì¶œ â†’ ë‹µë³€ ìƒì„±(ìë™ìœ¼ë¡œ historyì— ê¸°ë¡ë¨)\n",
        "    answer = rag_chain.invoke(\n",
        "        chain_input,\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# ì—¬ëŸ¬ê°œì˜ Chroma DBì— ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ -> ì• ë„ ì•ˆì”€\n",
        "def show_stored_documents():\n",
        "    file_names = set()\n",
        "\n",
        "    try:\n",
        "        for dept, dir_name in DEPARTMENT_MAP.items():\n",
        "            db = Chroma(\n",
        "                persist_directory=f\"/content/drive/MyDrive/ChromaDB/{dir_name}\",\n",
        "                embedding_function=hf_embeddings\n",
        "            )\n",
        "            result = db.get(include=[\"metadatas\"])\n",
        "            metas = result.get(\"metadatas\", [])\n",
        "            for meta in metas:\n",
        "                file_name = meta.get(\"file_name\", \"Unknown\")\n",
        "                department = meta.get(\"department\", dept)\n",
        "                file_names.add(f\"{file_name} ({department})\")\n",
        "    except Exception as e:\n",
        "        return f\"â— DBì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
        "\n",
        "    if not file_names:\n",
        "        return \"ğŸ“‚ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    return \"ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡:\\n\" + \"\\n\".join(f\"â€¢ {f}\" for f in sorted(file_names))\n",
        "\n",
        "# ì„¸ì…˜ë³„ë¡œ ì €ì¥ëœ ëŒ€í™” ë‚´ì—­ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜(Human/AI)\n",
        "def show_history(session_id: str) -> str:\n",
        "    init_session(session_id)\n",
        "    msgs = get_session_history(session_id).messages\n",
        "\n",
        "    if not msgs:\n",
        "        return \"âš ï¸ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    output = []\n",
        "    for msg in msgs:\n",
        "        if msg.type == \"system\":\n",
        "            output.append(f\"ğŸ“˜ System: {msg.content.strip()}\")\n",
        "        elif msg.type == \"human\":\n",
        "            output.append(f\"ğŸ§‘ Human: {msg.content.strip()}\")\n",
        "        elif msg.type == \"ai\":\n",
        "            output.append(f\"ğŸ¤– AI: {msg.content.strip()}\")\n",
        "    return \"\\n\\n\".join(output)\n",
        "\n",
        "# Gradio UI ì„¤ì •\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # â”€â”€ ì‚¬ì´ë“œë°”: ì„¸ì…˜ ê´€ë¦¬ â”€â”€\n",
        "    with gr.Sidebar():\n",
        "        session_dropdown = gr.Dropdown(\n",
        "            label=\"ì±„íŒ… ì„¸ì…˜ ì„ íƒ\",\n",
        "            choices=[],\n",
        "            value=None,\n",
        "            interactive=True\n",
        "        )\n",
        "        new_session_btn = gr.Button(\"â• ìƒˆ ì±„íŒ…\")\n",
        "        del_session_btn = gr.Button(\"ğŸ—‘ ì„¸ì…˜ ì‚­ì œ\")\n",
        "\n",
        "        department_dropdown = gr.Dropdown(\n",
        "            label=\"ğŸ” ê²€ìƒ‰í•  ë¶€ì„œ/í•™ê³¼ ì„ íƒ\",\n",
        "            choices=list(DEPARTMENT_MAP.keys()),\n",
        "            value=None,\n",
        "            interactive=True\n",
        "        )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ğŸ“„ ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡\n",
        "    **ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤!**\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            show_files_button = gr.Button(\"ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ë³´ê¸°\")\n",
        "            status_output = gr.Textbox(label=\"ğŸ“¢ ìƒíƒœ ë©”ì‹œì§€\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            question_input = gr.Textbox(label=\"â“ ì§ˆë¬¸ ì…ë ¥\", placeholder=\"ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì ì–´ì£¼ì„¸ìš”.\")\n",
        "            submit_button = gr.Button(\"ğŸ¤– ë‹µë³€ ë°›ê¸°\")\n",
        "            answer_output = gr.Textbox(label=\"ğŸ“ AI ë‹µë³€\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            show_history_button = gr.Button(\"ğŸ•˜ íˆìŠ¤í† ë¦¬ ë³´ê¸°\")\n",
        "            history_output = gr.Textbox(label=\"ğŸ—’ ì „ì²´ ëŒ€í™” ë‚´ì—­ | System/Human/AI Message\", lines=15, interactive=False)\n",
        "\n",
        "    # â”€â”€ ì‚¬ì´ë“œë°” ì´ë²¤íŠ¸ ë°”ì¸ë”© â”€â”€\n",
        "    def create_session():\n",
        "        sid = f\"session_{uuid.uuid4().hex[:8]}\"\n",
        "        init_session(sid)\n",
        "        return gr.update(choices=list(history_store.keys()), value=sid)\n",
        "\n",
        "    # ì„¸ì…˜ ì‚­ì œ í•¨ìˆ˜\n",
        "    def delete_session(session_id: str):\n",
        "        history_store.pop(session_id, None)\n",
        "\n",
        "    new_session_btn.click(\n",
        "        fn=create_session,\n",
        "        inputs=None,\n",
        "        outputs=[session_dropdown]\n",
        "    )\n",
        "\n",
        "    def remove_session(sid):\n",
        "        delete_session(sid)\n",
        "        keys = list(history_store.keys())\n",
        "        new_val = keys[0] if keys else None\n",
        "        return gr.update(choices=keys, value=new_val)\n",
        "\n",
        "    del_session_btn.click(\n",
        "        fn=remove_session,\n",
        "        inputs=[session_dropdown],\n",
        "        outputs=[session_dropdown]\n",
        "    )\n",
        "    session_dropdown.choices = list(history_store.keys())\n",
        "\n",
        "    # â”€â”€ ë¶€ì„œ ì„ íƒ ì‹œ RAG ì²´ì¸ ì¬êµ¬ì„± ì´ë²¤íŠ¸ ë°”ì¸ë”© â”€â”€\n",
        "    def on_department_selected(dept_name):\n",
        "        rebuild_chain(dept_name)\n",
        "        return f\"âœ… '{dept_name}' ë¶€ì„œ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ê¸°ë¥¼ ì¬ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    department_dropdown.change(\n",
        "        fn=on_department_selected,\n",
        "        inputs=[department_dropdown],\n",
        "        outputs=[status_output]\n",
        "    )\n",
        "\n",
        "    # â”€â”€ íŒŒì¼ ì—…ë¡œë“œ / ì§ˆë¬¸ / íˆìŠ¤í† ë¦¬ ì´ë²¤íŠ¸ â”€â”€\n",
        "    submit_button.click(answer_question, inputs=[question_input, session_dropdown], outputs=answer_output)\n",
        "    show_files_button.click(show_stored_documents, outputs=status_output)\n",
        "    show_history_button.click(show_history, inputs=[session_dropdown], outputs=history_output)\n",
        "\n",
        "# ë²¡í„° DB ë¡œë“œ í›„ ì‹¤í–‰\n",
        "rebuild_chain()\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7YUqoagH4Vr"
      },
      "outputs": [],
      "source": [
        "# ë“œë¼ì´ë¸Œ ë‚´ í´ë” ì•ˆì— ì¡´ì¬í•˜ëŠ” pdf íŒŒì¼ë“¤ ì €ì¥í•˜ëŠ” ì½”ë“œ\n",
        "# íë¦„: í´ë” ìˆœíšŒ -> pdf/md íŒŒì¼ 1ê°œ ì°¾ê¸° â†’ URL ë§¤ì¹­(ìœ ì‚¬ë„ ê¸°ë°˜) : íŒŒì¼ëª…ì„ ê°€ì§€ê³  URL ì•Œì•„ì˜¤ê¸° -> í…ìŠ¤íŠ¸ load & split(ì²­í‚¹) â†’ ë©”íƒ€ ì •ë³´ ì¶”ê°€ -> ChromaDBì— ì €ì¥(ìë™ ì„ë² ë”©)\n",
        "# íë¦„ì„ ë°˜ë³µí•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import logging\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "from io import BytesIO\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "# from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from google.cloud import vision\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# === Google Vision API ì„¤ì • === #\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Colab Notebooks/lively-sentry-460812-j2-b70abbe49963.json\"\n",
        "vision_client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# â€” ì„¤ì • â€” #\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/Cheonan Campus All Departments (ì²œì•ˆìº í¼ìŠ¤ ëª¨ë“  í•™ê³¼)/Department of Computer Engineering (ì»´í“¨í„°ê³µí•™ê³¼)\"   # ìˆœíšŒí•  ìµœìƒìœ„ í´ë”\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_computer\"         # ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬\n",
        "EMB_MODEL     = \"BAAI/bge-m3\"\n",
        "URL_XLSX_PATH = os.path.join(ROOT_FOLDER, \"Computer Department (ì»´í“¨í„°ê³µí•™ê³¼)_url.xlsx\")\n",
        "SIMILARITY_CUTOFF = 0.7\n",
        "\n",
        "# PyPDF ê²½ê³  ë¬´ì‹œ\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
        "\n",
        "# â€” ë¡œê¹… ì„¤ì • (Colabì—ì„œë„ ë³´ì´ê²Œ) â€” #\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "handler.setFormatter(formatter)\n",
        "if not logger.hasHandlers():\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "# íŒŒì¼ëª… ì •ê·œí™”: NFC â†’ í™•ì¥ì ì œê±° â†’ ê³µë°± ì œê±°\n",
        "def normalize_name(name: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFC\", name)\n",
        "    s = re.sub(r\"\\.\\w+$\", \"\", s)  # .pdf, .md ë“± ì œê±°\n",
        "    return s.replace(\" \", \"\")\n",
        "\n",
        "# 1) URL ë§¤í•‘ ë¡œë“œ\n",
        "def load_url_map(path: str) -> dict:\n",
        "    df = pd.read_excel(path)\n",
        "    return {\n",
        "        normalize_name(str(fn)): url\n",
        "        for fn, url in zip(df[\"íŒŒì¼ëª…\"], df[\"URL\"])\n",
        "    }\n",
        "URL_MAP = load_url_map(URL_XLSX_PATH)\n",
        "\n",
        "# 2) ê°€ì¥ ìœ ì‚¬í•œ URL ì°¾ì•„ì£¼ëŠ” í•¨ìˆ˜\n",
        "def find_best_url(base_norm: str, url_map: dict, cutoff: float) -> str:\n",
        "    # ì™„ì „ ì¼ì¹˜\n",
        "    if base_norm in url_map:\n",
        "        logger.info(f\"ğŸ”— URL 100% ì¼ì¹˜: '{base_norm}' â†’ {url_map[base_norm]}\")\n",
        "        return url_map[base_norm]\n",
        "\n",
        "    # ìœ ì‚¬ë„ íƒìƒ‰\n",
        "    best_key, best_score = None, 0.0\n",
        "    for key in url_map:\n",
        "        score = SequenceMatcher(None, base_norm, key).ratio()\n",
        "        if score > best_score:\n",
        "            best_key, best_score = key, score\n",
        "\n",
        "    if best_score >= cutoff:\n",
        "        logger.info(f\"ğŸ” ìœ ì‚¬ë„({best_score:.2f}) ë§¤ì¹­: '{base_norm}' â†’ '{best_key}' â†’ {url_map[best_key]}\")\n",
        "        return url_map[best_key]\n",
        "    else:\n",
        "        logger.info(f\"âŒ URL ë§¤ì¹­ ì‹¤íŒ¨({best_score:.2f}): '{base_norm}' â†’ URL ì—†ìŒ\")\n",
        "        return \"\"\n",
        "\n",
        "# 3) Loader ì„ íƒ (.pdf/.md)\n",
        "def get_loader(fp: Path):\n",
        "    ext = fp.suffix.lower()\n",
        "    if ext == \".pdf\":\n",
        "        return PyPDFLoader(str(fp))\n",
        "    if ext == \".md\":\n",
        "        return TextLoader(str(fp), encoding=\"utf-8\")\n",
        "    return None\n",
        "\n",
        "# OCR ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def ocr_pdf_to_text(pdf_path: str) -> str:\n",
        "    text_list = []\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path)\n",
        "        for i, image in enumerate(images):\n",
        "            img_byte_arr = BytesIO()\n",
        "            image.save(img_byte_arr, format='JPEG')\n",
        "            content = img_byte_arr.getvalue()\n",
        "            image_vision = vision.Image(content=content)\n",
        "            response = vision_client.document_text_detection(image=image_vision)\n",
        "            text = response.full_text_annotation.text\n",
        "            logger.info(f\"ğŸ“ OCR ì¶”ì¶œ (p.{i+1}): {len(text)}ì\")\n",
        "            text_list.append(text)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "    return \"\\n\".join(text_list)\n",
        "\n",
        "# 4) Splitter & Embedding ì¤€ë¹„\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        ")\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "\n",
        "# 5) ChromaDB ë¡œë“œ (í´ë”ê°€ ì—†ìœ¼ë©´ ë‚´ë¶€ì ìœ¼ë¡œ ìƒˆë¡œ ìƒì„±ë¨)\n",
        "db = Chroma(\n",
        "    persist_directory=PERSIST_DIR,\n",
        "    embedding_function=hf_embeddings\n",
        ")\n",
        "logger.info(\"ğŸ”„ ChromaDB ë¡œë“œ/ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "\n",
        "# 6) íŒŒì¼ ìˆœíšŒí•˜ë©° ì²­í‚¹ + ë©”íƒ€ ì¶”ê°€ + DB ì €ì¥\n",
        "for fp in Path(ROOT_FOLDER).rglob(\"*.*\"):\n",
        "    loader = get_loader(fp)\n",
        "    if loader is None:\n",
        "        continue\n",
        "\n",
        "    base_raw  = fp.name\n",
        "    base_norm = normalize_name(base_raw)\n",
        "    # department ì¶”ì¶œ (í˜„ì¬ ë£¨íŠ¸ í´ë” ê¸°ì¤€)\n",
        "    department = Path(ROOT_FOLDER).name\n",
        "\n",
        "    # ì¤‘ë³µ ì²´í¬: file_nameê³¼ departmentê°€ ëª¨ë‘ ì¼ì¹˜í•  ê²½ìš°ì—ë§Œ ì¤‘ë³µ ê°„ì£¼\n",
        "    try:\n",
        "        results = db.get(where={\"file_name\": {\"$eq\": base_raw}})\n",
        "        metadatas = results.get(\"metadatas\", [])\n",
        "        if any(md.get(\"department\") == department for md in metadatas):\n",
        "            logger.info(f\"â­ï¸ ì´ë¯¸ ì €ì¥ëœ íŒŒì¼: {base_raw} ({department}) â†’ ìŠ¤í‚µ\")\n",
        "            continue\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"â— ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}\")\n",
        "\n",
        "    logger.info(f\"ğŸ“„ íŒŒì¼ ë¡œë“œ & ì²­í‚¹ ì‹œì‘: {fp.relative_to(ROOT_FOLDER)}\")\n",
        "    url = find_best_url(base_norm, URL_MAP, SIMILARITY_CUTOFF)\n",
        "\n",
        "    # ë¡œë“œ & ì²­í‚¹\n",
        "    try:\n",
        "        chunks = loader.load_and_split(text_splitter=text_splitter)\n",
        "        logger.info(f\"ğŸ§© ì²­í¬ ìƒì„±: {len(chunks)}ê°œ\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ '{base_raw}' ë¡œë“œ/ì²­í‚¹ ì‹¤íŒ¨: {e}\")\n",
        "        continue\n",
        "\n",
        "    # ì²­í¬ê°€ 0ê°œë©´ ì‚¬ì§„ìœ¼ë¡œëœ pdfë¬¸ì„œì´ë¯€ë¡œ OCR ì‹œë„\n",
        "    if not chunks and fp.suffix.lower() == \".pdf\":\n",
        "        logger.warning(f\"âš ï¸ '{base_raw}' ë¹ˆ ì²­í¬ â†’ ì‚¬ì§„ìœ¼ë¡œ ëœ pdf\")\n",
        "        logger.info(f\"ğŸ–¼ï¸ OCR ì²˜ë¦¬ ì‹œë„: {base_raw}\")\n",
        "        ocr_text = ocr_pdf_to_text(str(fp))\n",
        "        if ocr_text.strip():\n",
        "            chunks = text_splitter.create_documents([ocr_text])\n",
        "        else:\n",
        "            logger.warning(f\"âš ï¸ OCR ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼: {base_raw}\")\n",
        "            continue\n",
        "\n",
        "    # ë©”íƒ€ ì¶”ê°€\n",
        "    for chunk in chunks:\n",
        "        chunk.metadata.update({\n",
        "            \"file_name\":  base_raw,\n",
        "            \"department\": department,\n",
        "            \"url\":        url\n",
        "        })\n",
        "\n",
        "    # DB ì €ì¥\n",
        "    db.add_documents(chunks)\n",
        "    logger.info(f\"â• '{base_raw}' â€” {len(chunks)}ê°œ Chroma DBì— ì €ì¥ ì™„ë£Œ\")\n",
        "\n",
        "logger.info(\"ğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ â€” ChromaDBì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
        "count = db._collection.count()\n",
        "logger.info(f\"í˜„ì¬ DB ì²­í¬í™”ëœ ë¬¸ì„œ ê°œìˆ˜: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQW_Hdd07pWS"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# â€” ì„¤ì • â€” #\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_computer\"\n",
        "EMB_MODEL   = \"BAAI/bge-m3\"\n",
        "\n",
        "# 1) DB ë¡œë“œ\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=hf_embeddings)\n",
        "\n",
        "# 2) ë©”íƒ€ì •ë³´ë§Œ êº¼ë‚´ì˜¤ê¸°\n",
        "res = db.get(include=[\"metadatas\"])\n",
        "\n",
        "# 3) ì´ ì²­í¬ ìˆ˜ ë° íŒŒì¼ëª…Â·ë¶€ì„œÂ·URL ì¶œë ¥\n",
        "metas = res[\"metadatas\"]\n",
        "print(f\"ì´ ì²­í¬ ìˆ˜: {len(metas)}\\n\")\n",
        "for meta in metas:\n",
        "    print(\n",
        "        f\"íŒŒì¼ëª…: {meta.get('file_name','Unknown')}  |  \"\n",
        "        f\"ë¶€ì„œ: {meta.get('department','Unknown')}  |  \"\n",
        "        f\"URL: {meta.get('url','')}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ-uYzmm_4XX",
        "outputId": "36f2a5c0-d506-462e-f066-eed3478baac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—‘ï¸ '/content/drive/MyDrive/ChromaDB/knu_chroma_db_computer' í´ë”ë¥¼ ì™„ì „íˆ ì œê±°í–ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "import shutil\n",
        "\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db_computer\"\n",
        "\n",
        "# 1) í´ë” ìì²´ë¥¼ ë‚ ë ¤ë²„ë¦¬ê¸°\n",
        "shutil.rmtree(PERSIST_DIR)\n",
        "\n",
        "print(f\"ğŸ—‘ï¸ '{PERSIST_DIR}' í´ë”ë¥¼ ì™„ì „íˆ ì œê±°í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sze8a2wqUg8m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "PERSIST_DIR = \"/content/drive/MyDrive/ChromaDB/knu_chroma_db\"\n",
        "EMB_MODEL   = \"BAAI/bge-m3\"\n",
        "\n",
        "# DB ë¡œë“œ\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=hf_embeddings)\n",
        "\n",
        "# 1. where ì ˆë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§\n",
        "try:\n",
        "    results = db.get(\n",
        "        where={\"department\": {\"$eq\": \"Department of Computer Engineering (ì»´í“¨í„°ê³µí•™ê³¼)\"}},\n",
        "        include=[\"metadatas\", \"documents\"]  # includeì— idsëŠ” ì“°ë©´ ì•ˆ ë¨\n",
        "    )\n",
        "    ids_to_delete = results[\"ids\"]  # ì—¬ê¸°ì„œ idê°€ ìë™ í¬í•¨ë¨\n",
        "\n",
        "    if ids_to_delete:\n",
        "        db.delete(ids=ids_to_delete)\n",
        "        print(f\"ğŸ—‘ï¸ ì‚­ì œëœ ì²­í¬ ìˆ˜: {len(ids_to_delete)}\")\n",
        "    else:\n",
        "        print(\"âœ… ì‚­ì œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "except Exception as e:\n",
        "    print(f\"â— ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" OCR ì‚¬ìš© ë°©ë²• ì˜ˆì œ\n",
        "from google.colab import drive\n",
        "import os\n",
        "from google.cloud import vision\n",
        "from pdf2image import convert_from_path\n",
        "from io import BytesIO\n",
        "\n",
        "# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œ\n",
        "# êµ¬ê¸€ ë¹„ì „ OCR api í‚¤\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Colab Notebooks/lively-sentry-460812-j2-b70abbe49963.json\"\n",
        "\n",
        "# Vision API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
        "vision_client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# OCR ëŒ€ìƒ PDF ê²½ë¡œ\n",
        "pdf_path = \"/content/drive/MyDrive/Cheonan Campus All Departments (ì²œì•ˆìº í¼ìŠ¤ ëª¨ë“  í•™ê³¼)/Software Department (ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼)/ì»¤ë®¤ë‹ˆí‹° (íŠ¹ê°•ê³µì§€)/ë³´ë¬¼ì˜ ì„¸ê³„ ì´ë²¤íŠ¸.pdf\"\n",
        "\n",
        "# PDF â†’ ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
        "images = convert_from_path(pdf_path)\n",
        "\n",
        "# ğŸš— 6. OCR ì²˜ë¦¬\n",
        "for i, image in enumerate(images):\n",
        "    img_byte_arr = BytesIO()\n",
        "    image.save(img_byte_arr, format='JPEG')\n",
        "    content = img_byte_arr.getvalue()\n",
        "\n",
        "    image_vision = vision.Image(content=content)\n",
        "    response = vision_client.document_text_detection(image=image_vision)\n",
        "    text = response.full_text_annotation.text\n",
        "    print(f\"ğŸ“ [í˜ì´ì§€ {i+1}] OCR ê²°ê³¼:\")\n",
        "    print(text)\n",
        "    print(\"-\" * 50)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IGM-u2-qU6X6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOd9j+BkfIDFHnEsw42ipeq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}